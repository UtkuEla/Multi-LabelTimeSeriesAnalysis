{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Conv1D, MaxPooling1D, Flatten, Dense, LSTM, Dropout, concatenate, GlobalMaxPooling1D\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.11.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPooling2D, Dropout, Rescaling\n",
    "from tensorflow.keras import Model\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense\n",
    "from keras.utils import to_categorical\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_pickle('total_data_pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = data[\"label\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "values = data.drop(columns=['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "values = values.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(values,labels, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))\n",
    "X_test = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (5471, 150, 1)\n",
      "X_test shape: (1368, 150, 1)\n",
      "y_train shape: (5471,)\n",
      "y_test shape: (1368,)\n"
     ]
    }
   ],
   "source": [
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"y_test shape:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_classes = len(np.unique(np.concatenate((y_train, y_test), axis=0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = X_train.shape[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = sklearn.preprocessing.OneHotEncoder(categories='auto')\n",
    "enc.fit(np.concatenate((y_train, y_test), axis=0).reshape(-1, 1))\n",
    "y_train = enc.transform(y_train.reshape(-1, 1)).toarray()\n",
    "y_test = enc.transform(y_test.reshape(-1, 1)).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150, 1)"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5471, 150, 1)"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "padding = 'valid'\n",
    "input_layer = keras.layers.Input(input_shape)\n",
    "\n",
    "conv1 = keras.layers.Conv1D(filters=6,kernel_size=7,padding=padding,activation='sigmoid')(input_layer)\n",
    "conv1 = keras.layers.AveragePooling1D(pool_size=3)(conv1)\n",
    "\n",
    "conv2 = keras.layers.Conv1D(filters=12,kernel_size=7,padding=padding,activation='sigmoid')(conv1)\n",
    "conv2 = keras.layers.AveragePooling1D(pool_size=3)(conv2)\n",
    "\n",
    "flatten_layer = keras.layers.Flatten()(conv2)\n",
    "\n",
    "output_layer = keras.layers.Dense(units=nb_classes,activation='softmax')(flatten_layer)\n",
    "\n",
    "model = keras.models.Model(inputs=input_layer, outputs=output_layer)\n",
    "\n",
    "model.compile(loss='mean_squared_error', optimizer=keras.optimizers.Adam(),\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "# model.compile(loss='categorical_crossentropy', optimizer=keras.optimizers.Adam(),\n",
    "#                 metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\UTKU\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:117: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(128, input_shape=input_shape, return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(64, return_sequences=False))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(nb_classes, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "optimizer = Adam(lr=0.001)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Concatenate, Input, Conv1D, MaxPooling1D, LSTM, GlobalMaxPooling1D, Dense, Dropout, Flatten\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "input_layer = Input(shape=input_shape)\n",
    "conv_layer1 = Conv1D(filters=32, kernel_size=3, activation='relu')(input_layer)\n",
    "conv_layer2 = Conv1D(filters=32, kernel_size=3, activation='relu')(conv_layer1)\n",
    "maxpool_layer = MaxPooling1D(pool_size=2)(conv_layer2)\n",
    "rnn_layer = LSTM(units=64, return_sequences=True)(maxpool_layer)\n",
    "cnn_layer = Conv1D(filters=64, kernel_size=3, activation='relu')(maxpool_layer)\n",
    "cnn_layer = MaxPooling1D(pool_size=2)(cnn_layer)\n",
    "cnn_layer = Conv1D(filters=64, kernel_size=3, activation='relu')(cnn_layer)\n",
    "cnn_layer = MaxPooling1D(pool_size=2)(cnn_layer)\n",
    "cnn_layer = Conv1D(filters=64, kernel_size=3, activation='relu')(cnn_layer)\n",
    "cnn_layer = MaxPooling1D(pool_size=2)(cnn_layer)\n",
    "cnn_layer = Flatten()(cnn_layer)\n",
    "rnn_layer = Flatten()(rnn_layer)\n",
    "rnn_layer = Dense(64)(rnn_layer)\n",
    "combined_layer = Concatenate()([rnn_layer, cnn_layer])\n",
    "global_pooling_layer = GlobalMaxPooling1D()(combined_layer[..., None])  # Add an extra dimension\n",
    "dense_layer = Dense(units=128, activation='relu')(global_pooling_layer)\n",
    "dropout_layer = Dropout(0.5)(dense_layer)\n",
    "output_layer = Dense(units=nb_classes, activation='softmax')(dropout_layer)\n",
    "\n",
    "model = Model(inputs=input_layer, outputs=output_layer)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=\"Adam\", metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, LSTM, Dense, Dropout\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=input_shape))\n",
    "model.add(Conv1D(filters=64, kernel_size=3, activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Conv1D(filters=128, kernel_size=3, activation='relu'))\n",
    "model.add(Conv1D(filters=128, kernel_size=3, activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(128))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(nb_classes, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Conv1D, MaxPooling1D, LSTM, Dense, GlobalMaxPooling1D, concatenate\n",
    "\n",
    "# Single input\n",
    "input_layer = Input(shape=input_shape)\n",
    "\n",
    "# Convolutional branch\n",
    "conv_branch = Conv1D(filters=64, kernel_size=3, activation='relu')(input_layer)\n",
    "conv_branch = MaxPooling1D(pool_size=2)(conv_branch)\n",
    "\n",
    "# Recurrent branch\n",
    "rnn_branch = LSTM(units=64, return_sequences=True)(input_layer)\n",
    "\n",
    "# Apply global max pooling to both branches\n",
    "conv_branch = GlobalMaxPooling1D()(conv_branch)\n",
    "rnn_branch = GlobalMaxPooling1D()(rnn_branch)\n",
    "\n",
    "# Concatenate the outputs of the convolutional and recurrent branches\n",
    "concatenated = concatenate([conv_branch, rnn_branch])\n",
    "\n",
    "# Classification layers\n",
    "dense_layer = Dense(units=128, activation='relu')(concatenated)\n",
    "output_layer = Dense(units=nb_classes, activation='softmax')(dense_layer)\n",
    "\n",
    "# Create the model\n",
    "model = Model(inputs=input_layer, outputs=output_layer)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Conv1D, MaxPooling1D, LSTM, Dense, GlobalMaxPooling1D, concatenate\n",
    "\n",
    "# Single input\n",
    "input_layer = Input(shape=input_shape)\n",
    "\n",
    "# Convolutional branch\n",
    "conv_branch = Conv1D(filters=64, kernel_size=3, activation='relu')(input_layer)\n",
    "conv_branch = MaxPooling1D(pool_size=2)(conv_branch)\n",
    "\n",
    "# First recurrent branch\n",
    "rnn_branch1 = LSTM(units=64, return_sequences=True)(input_layer)\n",
    "\n",
    "# Second recurrent branch\n",
    "rnn_branch2 = LSTM(units=64, return_sequences=True)(input_layer)\n",
    "\n",
    "# Apply global max pooling to the convolutional branch\n",
    "conv_branch = GlobalMaxPooling1D()(conv_branch)\n",
    "\n",
    "# Apply global max pooling to the recurrent branches\n",
    "rnn_branch1 = GlobalMaxPooling1D()(rnn_branch1)\n",
    "rnn_branch2 = GlobalMaxPooling1D()(rnn_branch2)\n",
    "\n",
    "# Concatenate the outputs of all branches\n",
    "concatenated = concatenate([conv_branch, rnn_branch1, rnn_branch2])\n",
    "\n",
    "# Classification layers\n",
    "dense_layer = Dense(units=128, activation='relu')(concatenated)\n",
    "output_layer = Dense(units=nb_classes, activation='softmax')(dense_layer)\n",
    "\n",
    "# Create the model\n",
    "model = Model(inputs=input_layer, outputs=output_layer)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "171/171 [==============================] - 11s 61ms/step - loss: 0.6772 - accuracy: 0.6077 - val_loss: 0.6930 - val_accuracy: 0.5965\n",
      "Epoch 2/500\n",
      "171/171 [==============================] - 11s 64ms/step - loss: 0.7601 - accuracy: 0.5924 - val_loss: 1.4550 - val_accuracy: 0.3918\n",
      "Epoch 3/500\n",
      "171/171 [==============================] - 11s 65ms/step - loss: 1.0331 - accuracy: 0.5357 - val_loss: 1.0166 - val_accuracy: 0.5914\n",
      "Epoch 4/500\n",
      "171/171 [==============================] - 11s 66ms/step - loss: 0.9328 - accuracy: 0.5710 - val_loss: 0.7791 - val_accuracy: 0.5892\n",
      "Epoch 5/500\n",
      "171/171 [==============================] - 12s 69ms/step - loss: 0.8781 - accuracy: 0.5778 - val_loss: 0.8023 - val_accuracy: 0.7449\n",
      "Epoch 6/500\n",
      "171/171 [==============================] - 12s 67ms/step - loss: 0.7416 - accuracy: 0.6200 - val_loss: 0.7624 - val_accuracy: 0.5928\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "batch_size = 32\n",
    "nb_epochs = 500\n",
    "\n",
    "# Define early stopping callback\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "# Compile the model\n",
    "#model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Train the model with optimizations\n",
    "hist = model.fit(X_train, y_train, batch_size=batch_size, epochs=nb_epochs,\n",
    "                 validation_data=(X_test, y_test), callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "171/171 [==============================] - 12s 68ms/step - loss: 0.1848 - accuracy: 0.9649 - val_loss: 0.2196 - val_accuracy: 0.9737\n",
      "Epoch 2/10\n",
      "171/171 [==============================] - 12s 70ms/step - loss: 0.2002 - accuracy: 0.9527 - val_loss: 0.2088 - val_accuracy: 0.9759\n",
      "Epoch 3/10\n",
      "171/171 [==============================] - 12s 70ms/step - loss: 0.1967 - accuracy: 0.9642 - val_loss: 0.2169 - val_accuracy: 0.9759\n",
      "Epoch 4/10\n",
      "171/171 [==============================] - 12s 70ms/step - loss: 0.1780 - accuracy: 0.9691 - val_loss: 0.2266 - val_accuracy: 0.9766\n",
      "Epoch 5/10\n",
      "171/171 [==============================] - 12s 70ms/step - loss: 0.1679 - accuracy: 0.9759 - val_loss: 0.2259 - val_accuracy: 0.9715\n",
      "Epoch 6/10\n",
      "171/171 [==============================] - 12s 70ms/step - loss: 0.1730 - accuracy: 0.9711 - val_loss: 0.2180 - val_accuracy: 0.9744\n",
      "Epoch 7/10\n",
      "171/171 [==============================] - 12s 70ms/step - loss: 0.1726 - accuracy: 0.9697 - val_loss: 0.2064 - val_accuracy: 0.9737\n",
      "Epoch 8/10\n",
      "171/171 [==============================] - 12s 70ms/step - loss: 0.1568 - accuracy: 0.9795 - val_loss: 0.2280 - val_accuracy: 0.9744\n",
      "Epoch 9/10\n",
      "171/171 [==============================] - 12s 70ms/step - loss: 0.1770 - accuracy: 0.9684 - val_loss: 0.2195 - val_accuracy: 0.9744\n",
      "Epoch 10/10\n",
      "171/171 [==============================] - 12s 70ms/step - loss: 0.1737 - accuracy: 0.9729 - val_loss: 0.3046 - val_accuracy: 0.9393\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "nb_epochs = 10\n",
    "\n",
    "hist = model.fit(X_train, y_train, batch_size=batch_size, epochs=nb_epochs, validation_data=(X_test, y_test))\n",
    "\n",
    "model.save('model1.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_values = pd.read_pickle('test_data_values')\n",
    "test_labels = pd.read_pickle('test_labels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>140</th>\n",
       "      <th>141</th>\n",
       "      <th>142</th>\n",
       "      <th>143</th>\n",
       "      <th>144</th>\n",
       "      <th>145</th>\n",
       "      <th>146</th>\n",
       "      <th>147</th>\n",
       "      <th>148</th>\n",
       "      <th>149</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.901586</td>\n",
       "      <td>-0.900917</td>\n",
       "      <td>-0.900318</td>\n",
       "      <td>-0.901755</td>\n",
       "      <td>-0.901106</td>\n",
       "      <td>-0.900660</td>\n",
       "      <td>-0.900371</td>\n",
       "      <td>-0.901117</td>\n",
       "      <td>-0.901063</td>\n",
       "      <td>-0.901148</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.887448</td>\n",
       "      <td>-0.887832</td>\n",
       "      <td>-0.887436</td>\n",
       "      <td>-0.886768</td>\n",
       "      <td>-0.886998</td>\n",
       "      <td>-0.887540</td>\n",
       "      <td>-0.886898</td>\n",
       "      <td>-0.886533</td>\n",
       "      <td>-0.886748</td>\n",
       "      <td>-0.886713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.887583</td>\n",
       "      <td>-0.888390</td>\n",
       "      <td>-0.888298</td>\n",
       "      <td>-0.888071</td>\n",
       "      <td>-0.887494</td>\n",
       "      <td>-0.887448</td>\n",
       "      <td>-0.887832</td>\n",
       "      <td>-0.887436</td>\n",
       "      <td>-0.886768</td>\n",
       "      <td>-0.886998</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.874551</td>\n",
       "      <td>-0.873345</td>\n",
       "      <td>-0.873314</td>\n",
       "      <td>-0.873625</td>\n",
       "      <td>-0.872445</td>\n",
       "      <td>-0.873875</td>\n",
       "      <td>-0.872376</td>\n",
       "      <td>-0.873233</td>\n",
       "      <td>-0.873090</td>\n",
       "      <td>-0.873190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.874648</td>\n",
       "      <td>-0.874375</td>\n",
       "      <td>-0.874732</td>\n",
       "      <td>-0.874544</td>\n",
       "      <td>-0.873840</td>\n",
       "      <td>-0.874551</td>\n",
       "      <td>-0.873345</td>\n",
       "      <td>-0.873314</td>\n",
       "      <td>-0.873625</td>\n",
       "      <td>-0.872445</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.860759</td>\n",
       "      <td>-0.859499</td>\n",
       "      <td>-0.859683</td>\n",
       "      <td>-0.860375</td>\n",
       "      <td>-0.859775</td>\n",
       "      <td>-0.859690</td>\n",
       "      <td>-0.859191</td>\n",
       "      <td>-0.859544</td>\n",
       "      <td>-0.859921</td>\n",
       "      <td>-0.859771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.859384</td>\n",
       "      <td>-0.860256</td>\n",
       "      <td>-0.860844</td>\n",
       "      <td>-0.860487</td>\n",
       "      <td>-0.859506</td>\n",
       "      <td>-0.860759</td>\n",
       "      <td>-0.859499</td>\n",
       "      <td>-0.859683</td>\n",
       "      <td>-0.860375</td>\n",
       "      <td>-0.859775</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.847159</td>\n",
       "      <td>-0.847298</td>\n",
       "      <td>-0.846806</td>\n",
       "      <td>-0.846990</td>\n",
       "      <td>-0.847605</td>\n",
       "      <td>-0.847051</td>\n",
       "      <td>-0.846398</td>\n",
       "      <td>-0.847267</td>\n",
       "      <td>-0.846659</td>\n",
       "      <td>-0.846171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.847825</td>\n",
       "      <td>-0.847828</td>\n",
       "      <td>-0.847632</td>\n",
       "      <td>-0.847571</td>\n",
       "      <td>-0.847232</td>\n",
       "      <td>-0.847159</td>\n",
       "      <td>-0.847298</td>\n",
       "      <td>-0.846806</td>\n",
       "      <td>-0.846990</td>\n",
       "      <td>-0.847605</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.833187</td>\n",
       "      <td>-0.833398</td>\n",
       "      <td>-0.833798</td>\n",
       "      <td>-0.833571</td>\n",
       "      <td>-0.832514</td>\n",
       "      <td>-0.832529</td>\n",
       "      <td>-0.832406</td>\n",
       "      <td>-0.832191</td>\n",
       "      <td>-0.832414</td>\n",
       "      <td>-0.831760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4095</th>\n",
       "      <td>-1.013981</td>\n",
       "      <td>-0.994308</td>\n",
       "      <td>-0.993894</td>\n",
       "      <td>-0.990793</td>\n",
       "      <td>-1.019035</td>\n",
       "      <td>-0.996026</td>\n",
       "      <td>-0.983637</td>\n",
       "      <td>-0.996609</td>\n",
       "      <td>-0.996955</td>\n",
       "      <td>-1.011656</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.987845</td>\n",
       "      <td>-0.995404</td>\n",
       "      <td>-1.004695</td>\n",
       "      <td>-0.993447</td>\n",
       "      <td>-1.003347</td>\n",
       "      <td>-0.999714</td>\n",
       "      <td>-1.011468</td>\n",
       "      <td>-1.001956</td>\n",
       "      <td>-1.000227</td>\n",
       "      <td>-0.991298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4096</th>\n",
       "      <td>-1.016446</td>\n",
       "      <td>-0.994438</td>\n",
       "      <td>-1.004724</td>\n",
       "      <td>-1.010113</td>\n",
       "      <td>-1.007526</td>\n",
       "      <td>-0.987845</td>\n",
       "      <td>-0.995404</td>\n",
       "      <td>-1.004695</td>\n",
       "      <td>-0.993447</td>\n",
       "      <td>-1.003347</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.991836</td>\n",
       "      <td>-1.004428</td>\n",
       "      <td>-1.000935</td>\n",
       "      <td>-1.008884</td>\n",
       "      <td>-1.007629</td>\n",
       "      <td>-1.011201</td>\n",
       "      <td>-1.005180</td>\n",
       "      <td>-1.012828</td>\n",
       "      <td>-1.008138</td>\n",
       "      <td>-1.000120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4097</th>\n",
       "      <td>-0.989538</td>\n",
       "      <td>-1.015580</td>\n",
       "      <td>-1.002616</td>\n",
       "      <td>-0.996496</td>\n",
       "      <td>-1.004726</td>\n",
       "      <td>-0.991836</td>\n",
       "      <td>-1.004428</td>\n",
       "      <td>-1.000935</td>\n",
       "      <td>-1.008884</td>\n",
       "      <td>-1.007629</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.020212</td>\n",
       "      <td>-1.002727</td>\n",
       "      <td>-1.014782</td>\n",
       "      <td>-1.004433</td>\n",
       "      <td>-0.989422</td>\n",
       "      <td>-1.005432</td>\n",
       "      <td>-1.015047</td>\n",
       "      <td>-0.993692</td>\n",
       "      <td>-1.003962</td>\n",
       "      <td>-0.999594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4098</th>\n",
       "      <td>-0.995656</td>\n",
       "      <td>-1.003081</td>\n",
       "      <td>-1.001588</td>\n",
       "      <td>-1.002686</td>\n",
       "      <td>-0.993313</td>\n",
       "      <td>-1.020212</td>\n",
       "      <td>-1.002727</td>\n",
       "      <td>-1.014782</td>\n",
       "      <td>-1.004433</td>\n",
       "      <td>-0.989422</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.008192</td>\n",
       "      <td>-1.007724</td>\n",
       "      <td>-0.999536</td>\n",
       "      <td>-1.002337</td>\n",
       "      <td>-0.999768</td>\n",
       "      <td>-0.997948</td>\n",
       "      <td>-1.003441</td>\n",
       "      <td>-1.008146</td>\n",
       "      <td>-1.010657</td>\n",
       "      <td>-1.001110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4099</th>\n",
       "      <td>-0.952643</td>\n",
       "      <td>-0.967768</td>\n",
       "      <td>-0.858705</td>\n",
       "      <td>-0.898325</td>\n",
       "      <td>-0.935685</td>\n",
       "      <td>-0.939404</td>\n",
       "      <td>-0.774405</td>\n",
       "      <td>-0.733363</td>\n",
       "      <td>-0.921513</td>\n",
       "      <td>-0.896960</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.898962</td>\n",
       "      <td>-0.743273</td>\n",
       "      <td>-0.987308</td>\n",
       "      <td>-0.969541</td>\n",
       "      <td>-0.764335</td>\n",
       "      <td>-0.762658</td>\n",
       "      <td>-0.750287</td>\n",
       "      <td>-0.841057</td>\n",
       "      <td>-0.894229</td>\n",
       "      <td>-0.698756</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4100 rows × 150 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           0         1         2         3         4         5         6    \\\n",
       "0    -0.901586 -0.900917 -0.900318 -0.901755 -0.901106 -0.900660 -0.900371   \n",
       "1    -0.887583 -0.888390 -0.888298 -0.888071 -0.887494 -0.887448 -0.887832   \n",
       "2    -0.874648 -0.874375 -0.874732 -0.874544 -0.873840 -0.874551 -0.873345   \n",
       "3    -0.859384 -0.860256 -0.860844 -0.860487 -0.859506 -0.860759 -0.859499   \n",
       "4    -0.847825 -0.847828 -0.847632 -0.847571 -0.847232 -0.847159 -0.847298   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "4095 -1.013981 -0.994308 -0.993894 -0.990793 -1.019035 -0.996026 -0.983637   \n",
       "4096 -1.016446 -0.994438 -1.004724 -1.010113 -1.007526 -0.987845 -0.995404   \n",
       "4097 -0.989538 -1.015580 -1.002616 -0.996496 -1.004726 -0.991836 -1.004428   \n",
       "4098 -0.995656 -1.003081 -1.001588 -1.002686 -0.993313 -1.020212 -1.002727   \n",
       "4099 -0.952643 -0.967768 -0.858705 -0.898325 -0.935685 -0.939404 -0.774405   \n",
       "\n",
       "           7         8         9    ...       140       141       142  \\\n",
       "0    -0.901117 -0.901063 -0.901148  ... -0.887448 -0.887832 -0.887436   \n",
       "1    -0.887436 -0.886768 -0.886998  ... -0.874551 -0.873345 -0.873314   \n",
       "2    -0.873314 -0.873625 -0.872445  ... -0.860759 -0.859499 -0.859683   \n",
       "3    -0.859683 -0.860375 -0.859775  ... -0.847159 -0.847298 -0.846806   \n",
       "4    -0.846806 -0.846990 -0.847605  ... -0.833187 -0.833398 -0.833798   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "4095 -0.996609 -0.996955 -1.011656  ... -0.987845 -0.995404 -1.004695   \n",
       "4096 -1.004695 -0.993447 -1.003347  ... -0.991836 -1.004428 -1.000935   \n",
       "4097 -1.000935 -1.008884 -1.007629  ... -1.020212 -1.002727 -1.014782   \n",
       "4098 -1.014782 -1.004433 -0.989422  ... -1.008192 -1.007724 -0.999536   \n",
       "4099 -0.733363 -0.921513 -0.896960  ... -0.898962 -0.743273 -0.987308   \n",
       "\n",
       "           143       144       145       146       147       148       149  \n",
       "0    -0.886768 -0.886998 -0.887540 -0.886898 -0.886533 -0.886748 -0.886713  \n",
       "1    -0.873625 -0.872445 -0.873875 -0.872376 -0.873233 -0.873090 -0.873190  \n",
       "2    -0.860375 -0.859775 -0.859690 -0.859191 -0.859544 -0.859921 -0.859771  \n",
       "3    -0.846990 -0.847605 -0.847051 -0.846398 -0.847267 -0.846659 -0.846171  \n",
       "4    -0.833571 -0.832514 -0.832529 -0.832406 -0.832191 -0.832414 -0.831760  \n",
       "...        ...       ...       ...       ...       ...       ...       ...  \n",
       "4095 -0.993447 -1.003347 -0.999714 -1.011468 -1.001956 -1.000227 -0.991298  \n",
       "4096 -1.008884 -1.007629 -1.011201 -1.005180 -1.012828 -1.008138 -1.000120  \n",
       "4097 -1.004433 -0.989422 -1.005432 -1.015047 -0.993692 -1.003962 -0.999594  \n",
       "4098 -1.002337 -0.999768 -0.997948 -1.003441 -1.008146 -1.010657 -1.001110  \n",
       "4099 -0.969541 -0.764335 -0.762658 -0.750287 -0.841057 -0.894229 -0.698756  \n",
       "\n",
       "[4100 rows x 150 columns]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_values = test_values.values\n",
    "test_values = test_values.reshape((test_values.shape[0], test_values.shape[1], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4100, 150, 1)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "129/129 [==============================] - 3s 22ms/step\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict(test_values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[9.9985027e-01, 1.2970601e-04, 0.0000000e+00, 2.6459085e-10,\n",
       "        8.0338548e-12, 1.9981904e-05],\n",
       "       [9.9703169e-01, 2.9662673e-03, 0.0000000e+00, 3.0329730e-12,\n",
       "        8.1376648e-11, 1.9991078e-06],\n",
       "       [3.6086941e-01, 6.3913035e-01, 0.0000000e+00, 2.1593414e-13,\n",
       "        4.1875867e-09, 2.2367858e-07],\n",
       "       ...,\n",
       "       [2.4517945e-09, 1.5257289e-05, 1.4485131e-04, 1.8355233e-05,\n",
       "        1.8492981e-11, 9.9982160e-01],\n",
       "       [1.2897338e-09, 2.8804047e-06, 8.1640216e-08, 8.6834768e-07,\n",
       "        6.0279407e-12, 9.9999619e-01],\n",
       "       [4.3627072e-08, 2.0809578e-11, 0.0000000e+00, 0.0000000e+00,\n",
       "        7.1268962e-13, 1.0000000e+00]], dtype=float32)"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                Prediction Prediction2\n",
      "0                     Bias     NoFault\n",
      "1                     Bias     NoFault\n",
      "2                    Drift        Bias\n",
      "3                    Drift     NoFault\n",
      "4                    Drift     NoFault\n",
      "...                    ...         ...\n",
      "4095  Precisiondegradation     NoFault\n",
      "4096  Precisiondegradation     NoFault\n",
      "4097  Precisiondegradation     NoFault\n",
      "4098  Precisiondegradation     NoFault\n",
      "4099  Precisiondegradation     NoFault\n",
      "\n",
      "[4100 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "prediction_list = []\n",
    "\n",
    "threshold = 0.05\n",
    "\n",
    "# Iterate over each row of predictions\n",
    "for row in predictions:\n",
    "    # Sort the probabilities in descending order and get the indices\n",
    "    sorted_indices = np.argsort(row)[::-1]\n",
    "\n",
    "    # Check if the highest prediction is above the threshold\n",
    "    if row[sorted_indices[0]] >= threshold:\n",
    "        prediction1 = sorted_indices[0]\n",
    "    else:\n",
    "        prediction1 = 3\n",
    "\n",
    "    # Check if the second highest prediction is above the threshold\n",
    "    if row[sorted_indices[1]] >= threshold:\n",
    "        prediction2 = sorted_indices[1]\n",
    "    else:\n",
    "        prediction2 = 3\n",
    "\n",
    "    # Add the predictions to the list\n",
    "    prediction_list.append([prediction1, prediction2])\n",
    "\n",
    "# Create the DataFrame from the list\n",
    "#df = pd.DataFrame(prediction_list, columns=['prediction1', 'prediction2'])\n",
    "prediction_list = np.array(prediction_list)\n",
    "labels = ['Bias', 'Drift', 'Gain', 'NoFault', 'Outliers', 'Precisiondegradation']\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(labels)\n",
    "predicted_labels_1 = label_encoder.inverse_transform(prediction_list[:,0])\n",
    "predicted_labels_2 = label_encoder.inverse_transform(prediction_list[:,1])\n",
    "df_predictions = pd.DataFrame({'Prediction': predicted_labels_1, 'Prediction2': predicted_labels_2})\n",
    "\n",
    "# Display the resulting DataFrame\n",
    "print(df_predictions)\n",
    "df_predictions.to_excel('predictions1.xlsx')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.0171706 , 0.9817509 ],\n",
       "       [0.02506384, 0.9733617 ],\n",
       "       [0.03759037, 0.95918125],\n",
       "       ...,\n",
       "       [0.32750604, 0.45511836],\n",
       "       [0.34040767, 0.47297347],\n",
       "       [0.04950599, 0.942229  ]], dtype=float32)"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_two_probs = np.sort(predictions, axis=1)[:, -2:]\n",
    "top_two_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Prediction</th>\n",
       "      <th>Prediction2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bias</td>\n",
       "      <td>Drift</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bias</td>\n",
       "      <td>Drift</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Drift</td>\n",
       "      <td>Bias</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Drift</td>\n",
       "      <td>Bias</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Drift</td>\n",
       "      <td>Bias</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4095</th>\n",
       "      <td>Precisiondegradation</td>\n",
       "      <td>Gain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4096</th>\n",
       "      <td>Precisiondegradation</td>\n",
       "      <td>Gain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4097</th>\n",
       "      <td>Precisiondegradation</td>\n",
       "      <td>Gain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4098</th>\n",
       "      <td>Precisiondegradation</td>\n",
       "      <td>Gain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4099</th>\n",
       "      <td>Precisiondegradation</td>\n",
       "      <td>Bias</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4100 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                Prediction Prediction2\n",
       "0                     Bias       Drift\n",
       "1                     Bias       Drift\n",
       "2                    Drift        Bias\n",
       "3                    Drift        Bias\n",
       "4                    Drift        Bias\n",
       "...                    ...         ...\n",
       "4095  Precisiondegradation        Gain\n",
       "4096  Precisiondegradation        Gain\n",
       "4097  Precisiondegradation        Gain\n",
       "4098  Precisiondegradation        Gain\n",
       "4099  Precisiondegradation        Bias\n",
       "\n",
       "[4100 rows x 2 columns]"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_two_indices = np.argsort(predictions, axis=1)[:, -2:]\n",
    "predicted_label_indices = top_two_indices[:, ::-1]\n",
    "first_predictions = predicted_label_indices[:,0]\n",
    "second_predictions = predicted_label_indices[:,1]\n",
    "labels = ['Bias', 'Drift', 'Gain', 'NoFault', 'Outliers', 'Precisiondegradation']\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(labels)\n",
    "predicted_labels_1 = label_encoder.inverse_transform(first_predictions)\n",
    "predicted_labels_2 = label_encoder.inverse_transform(second_predictions)\n",
    "df_predictions = pd.DataFrame({'Prediction': predicted_labels_1, 'Prediction2': predicted_labels_2})\n",
    "\n",
    "df_predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_predictions.to_excel('predictions.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Bias', 'Bias', 'Bias', ..., 'Gain', 'Gain', 'NoFault'],\n",
       "      dtype='<U20')"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = ['Bias', 'Drift', 'Gain', 'NoFault', 'Outliers', 'Precisiondegradation']\n",
    "predicted_labels = np.argmax(predictions, axis=1)\n",
    "predicted_label_indices = np.argmax(predictions, axis=1)\n",
    "\n",
    "# top_two_indices = np.argsort(predictions, axis=1)[:, -2:]\n",
    "# predicted_label_indices = top_two_indices[:, ::-1]\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(labels)\n",
    "\n",
    "predicted_labels = label_encoder.inverse_transform(predicted_label_indices)\n",
    "\n",
    "predicted_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_two_probs = np.sort(predictions, axis=1)[:, -2:]\n",
    "threshold = 0.5  # Adjust this threshold according to your needs\n",
    "\n",
    "predicted_labels = np.zeros((predictions.shape[0], 2), dtype=np.int32)\n",
    "predicted_labels[:, 0] = predicted_label_indices\n",
    "\n",
    "mask = top_two_probs[:, 1] > threshold\n",
    "predicted_labels[mask, 1] = predicted_label_indices[mask]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_two_probs = np.sort(predictions, axis=1)[:, -2:]\n",
    "threshold = 0.5  # Adjust this threshold according to your needs\n",
    "\n",
    "predicted_labels = np.zeros((predictions.shape[0], 1), dtype=np.int32)\n",
    "predicted_labels[:, 0] = predicted_label_indices[:predicted_labels.shape[0]]\n",
    "\n",
    "mask = top_two_probs[:, 1] > threshold\n",
    "\n",
    "predicted_labels_2 = np.zeros((predicted_labels.shape[0], 1), dtype=np.int32)\n",
    "predicted_labels_2[mask] = predicted_label_indices[mask].reshape(-1, 1)\n",
    "\n",
    "predicted_labels = np.concatenate((predicted_labels, predicted_labels_2), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Prediction Prediction2\n",
      "0          Bias        Bias\n",
      "1          Bias        Bias\n",
      "2          Bias        Bias\n",
      "3       NoFault        Bias\n",
      "4       NoFault        Bias\n",
      "...         ...         ...\n",
      "4095       Gain        Bias\n",
      "4096       Gain        Bias\n",
      "4097       Gain        Bias\n",
      "4098       Gain        Bias\n",
      "4099    NoFault        Bias\n",
      "\n",
      "[4100 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "prediction_column = label_encoder.inverse_transform(predicted_labels[:, 0])\n",
    "prediction2_column = label_encoder.inverse_transform(predicted_labels[:, 1])\n",
    "\n",
    "predictions_df = pd.DataFrame({'Prediction': prediction_column, 'Prediction2': prediction2_column})\n",
    "print(predictions_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_df.to_excel('predictions.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 3, 3, 0], dtype=int64)"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_label_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_labels_df = pd.DataFrame(predicted_labels)\n",
    "predicted_labels_df.to_excel('predictions.xlsx')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
