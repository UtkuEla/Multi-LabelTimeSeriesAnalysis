{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Conv1D, MaxPooling1D, Flatten, Dense, LSTM, Dropout, concatenate, GlobalMaxPooling1D\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.11.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPooling2D, Dropout, Rescaling\n",
    "from tensorflow.keras import Model\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense\n",
    "from keras.utils import to_categorical\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_pickle('total_data_pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 300, 1)]     0           []                               \n",
      "                                                                                                  \n",
      " conv1d (Conv1D)                (None, 298, 128)     512         ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " conv1d_1 (Conv1D)              (None, 296, 64)      24640       ['conv1d[0][0]']                 \n",
      "                                                                                                  \n",
      " max_pooling1d (MaxPooling1D)   (None, 148, 64)      0           ['conv1d_1[0][0]']               \n",
      "                                                                                                  \n",
      " lstm (LSTM)                    (None, 298, 64)      49408       ['conv1d[0][0]']                 \n",
      "                                                                                                  \n",
      " lstm_1 (LSTM)                  (None, 300, 64)      16896       ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " global_max_pooling1d (GlobalMa  (None, 64)          0           ['max_pooling1d[0][0]']          \n",
      " xPooling1D)                                                                                      \n",
      "                                                                                                  \n",
      " global_max_pooling1d_1 (Global  (None, 64)          0           ['lstm[0][0]']                   \n",
      " MaxPooling1D)                                                                                    \n",
      "                                                                                                  \n",
      " global_max_pooling1d_2 (Global  (None, 64)          0           ['lstm_1[0][0]']                 \n",
      " MaxPooling1D)                                                                                    \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 192)          0           ['global_max_pooling1d[0][0]',   \n",
      "                                                                  'global_max_pooling1d_1[0][0]', \n",
      "                                                                  'global_max_pooling1d_2[0][0]'] \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 128)          24704       ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 6)            774         ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 116,934\n",
      "Trainable params: 116,934\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.models.load_model(\"model_parallel_azad.hdf5\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.engine.functional.Functional at 0x135bd0ffb20>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = data[\"label\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values = data.drop(columns=['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from testdata import *\n",
    "# from traindata import *\n",
    "\n",
    "\n",
    "# sampleSize = 300\n",
    "# overlapRatio = 10\n",
    "# path_train = \"df_f_feather\"\n",
    "# path_test = \"test_df\"\n",
    "\n",
    "# train = trainData()\n",
    "# test = testData()\n",
    "\n",
    "\n",
    "# values, labels = train.prepareTrainData(sampleSize, overlapRatio, path_train)\n",
    "# test_values, test_labels = test.prepareTestData(sampleSize, overlapRatio, path_test)\n",
    "\n",
    "current_folder_path = os.getcwd()\n",
    "parent_folder_path = os.path.dirname(current_folder_path)\n",
    "data_save_path = os.path.join(parent_folder_path, 'data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = parent_folder_path + \"\\\\train_data_values_300_10\"\n",
    "values= pd.read_pickle(path)\n",
    "path = parent_folder_path + \"\\\\train_labels_300_10\"\n",
    "labels= pd.read_pickle(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = labels.values\n",
    "values = values.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(values,labels, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))\n",
    "X_test = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (2735, 300, 1)\n",
      "X_test shape: (684, 300, 1)\n",
      "y_train shape: (2735, 1)\n",
      "y_test shape: (684, 1)\n"
     ]
    }
   ],
   "source": [
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"y_test shape:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_classes = len(np.unique(np.concatenate((y_train, y_test), axis=0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = X_train.shape[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = sklearn.preprocessing.OneHotEncoder(categories='auto')\n",
    "enc.fit(np.concatenate((y_train, y_test), axis=0).reshape(-1, 1))\n",
    "y_train = enc.transform(y_train.reshape(-1, 1)).toarray()\n",
    "y_test = enc.transform(y_test.reshape(-1, 1)).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300, 1)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2735, 300, 1)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "padding = 'valid'\n",
    "input_layer = keras.layers.Input(input_shape)\n",
    "\n",
    "conv1 = keras.layers.Conv1D(filters=6,kernel_size=7,padding=padding,activation='sigmoid')(input_layer)\n",
    "conv1 = keras.layers.AveragePooling1D(pool_size=3)(conv1)\n",
    "\n",
    "conv2 = keras.layers.Conv1D(filters=12,kernel_size=7,padding=padding,activation='sigmoid')(conv1)\n",
    "conv2 = keras.layers.AveragePooling1D(pool_size=3)(conv2)\n",
    "\n",
    "flatten_layer = keras.layers.Flatten()(conv2)\n",
    "\n",
    "output_layer = keras.layers.Dense(units=nb_classes,activation='softmax')(flatten_layer)\n",
    "\n",
    "model = keras.models.Model(inputs=input_layer, outputs=output_layer)\n",
    "\n",
    "model.compile(loss='mean_squared_error', optimizer=keras.optimizers.Adam(),\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "# model.compile(loss='categorical_crossentropy', optimizer=keras.optimizers.Adam(),\n",
    "#                 metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(128, input_shape=input_shape, return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(64, return_sequences=False))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(nb_classes, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "optimizer = Adam(lr=0.001)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Concatenate, Input, Conv1D, MaxPooling1D, LSTM, GlobalMaxPooling1D, Dense, Dropout, Flatten\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "input_layer = Input(shape=input_shape)\n",
    "conv_layer1 = Conv1D(filters=32, kernel_size=3, activation='relu')(input_layer)\n",
    "conv_layer2 = Conv1D(filters=32, kernel_size=3, activation='relu')(conv_layer1)\n",
    "maxpool_layer = MaxPooling1D(pool_size=2)(conv_layer2)\n",
    "rnn_layer = LSTM(units=64, return_sequences=True)(maxpool_layer)\n",
    "cnn_layer = Conv1D(filters=64, kernel_size=3, activation='relu')(maxpool_layer)\n",
    "cnn_layer = MaxPooling1D(pool_size=2)(cnn_layer)\n",
    "cnn_layer = Conv1D(filters=64, kernel_size=3, activation='relu')(cnn_layer)\n",
    "cnn_layer = MaxPooling1D(pool_size=2)(cnn_layer)\n",
    "cnn_layer = Conv1D(filters=64, kernel_size=3, activation='relu')(cnn_layer)\n",
    "cnn_layer = MaxPooling1D(pool_size=2)(cnn_layer)\n",
    "cnn_layer = Flatten()(cnn_layer)\n",
    "rnn_layer = Flatten()(rnn_layer)\n",
    "rnn_layer = Dense(64)(rnn_layer)\n",
    "combined_layer = Concatenate()([rnn_layer, cnn_layer])\n",
    "global_pooling_layer = GlobalMaxPooling1D()(combined_layer[..., None])  # Add an extra dimension\n",
    "dense_layer = Dense(units=128, activation='relu')(global_pooling_layer)\n",
    "dropout_layer = Dropout(0.5)(dense_layer)\n",
    "output_layer = Dense(units=nb_classes, activation='softmax')(dropout_layer)\n",
    "\n",
    "model = Model(inputs=input_layer, outputs=output_layer)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=\"Adam\", metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, LSTM, Dense, Dropout\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=input_shape))\n",
    "model.add(Conv1D(filters=64, kernel_size=3, activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Conv1D(filters=128, kernel_size=3, activation='relu'))\n",
    "model.add(Conv1D(filters=128, kernel_size=3, activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(128))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(nb_classes, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Conv1D, MaxPooling1D, LSTM, Dense, GlobalMaxPooling1D, concatenate\n",
    "\n",
    "# Single input\n",
    "input_layer = Input(shape=input_shape)\n",
    "\n",
    "# Convolutional branch\n",
    "conv_branch = Conv1D(filters=64, kernel_size=3, activation='relu')(input_layer)\n",
    "conv_branch = MaxPooling1D(pool_size=2)(conv_branch)\n",
    "\n",
    "# Recurrent branch\n",
    "rnn_branch = LSTM(units=64, return_sequences=True)(input_layer)\n",
    "\n",
    "# Apply global max pooling to both branches\n",
    "conv_branch = GlobalMaxPooling1D()(conv_branch)\n",
    "rnn_branch = GlobalMaxPooling1D()(rnn_branch)\n",
    "\n",
    "# Concatenate the outputs of the convolutional and recurrent branches\n",
    "concatenated = concatenate([conv_branch, rnn_branch])\n",
    "\n",
    "# Classification layers\n",
    "dense_layer = Dense(units=128, activation='relu')(concatenated)\n",
    "output_layer = Dense(units=nb_classes, activation='softmax')(dense_layer)\n",
    "\n",
    "# Create the model\n",
    "model = Model(inputs=input_layer, outputs=output_layer)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Conv1D, MaxPooling1D, LSTM, Dense, GlobalMaxPooling1D, concatenate\n",
    "\n",
    "# Single input\n",
    "input_layer = Input(shape=input_shape)\n",
    "\n",
    "# Convolutional branch\n",
    "conv_branch = Conv1D(filters=64, kernel_size=3, activation='relu')(input_layer)\n",
    "conv_branch = MaxPooling1D(pool_size=2)(conv_branch)\n",
    "\n",
    "# First recurrent branch\n",
    "rnn_branch1 = LSTM(units=64, return_sequences=True)(input_layer)\n",
    "\n",
    "# Second recurrent branch\n",
    "rnn_branch2 = LSTM(units=64, return_sequences=True)(input_layer)\n",
    "\n",
    "# Apply global max pooling to the convolutional branch\n",
    "conv_branch = GlobalMaxPooling1D()(conv_branch)\n",
    "\n",
    "# Apply global max pooling to the recurrent branches\n",
    "rnn_branch1 = GlobalMaxPooling1D()(rnn_branch1)\n",
    "rnn_branch2 = GlobalMaxPooling1D()(rnn_branch2)\n",
    "\n",
    "# Concatenate the outputs of all branches\n",
    "concatenated = concatenate([conv_branch, rnn_branch1, rnn_branch2])\n",
    "\n",
    "# Classification layers\n",
    "dense_layer = Dense(units=128, activation='relu')(concatenated)\n",
    "output_layer = Dense(units=nb_classes, activation='softmax')(dense_layer)\n",
    "\n",
    "# Create the model\n",
    "model = Model(inputs=input_layer, outputs=output_layer)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Conv1D, MaxPooling1D, LSTM, Dense, GlobalMaxPooling1D, concatenate\n",
    "\n",
    "# Single input\n",
    "input_layer = Input(shape=input_shape)\n",
    "\n",
    "conv_branch1 = Conv1D(filters=128, kernel_size=3, activation='relu')(input_layer)\n",
    "\n",
    "# Convolutional branch\n",
    "conv_branch = Conv1D(filters=64, kernel_size=3, activation='relu')(conv_branch1)\n",
    "conv_branch = MaxPooling1D(pool_size=2)(conv_branch)\n",
    "\n",
    "# First recurrent branch\n",
    "rnn_branch1 = LSTM(units=64, return_sequences=True)(conv_branch1)\n",
    "\n",
    "# Second recurrent branch\n",
    "rnn_branch2 = LSTM(units=64, return_sequences=True)(input_layer)\n",
    "\n",
    "# Apply global max pooling to the convolutional branch\n",
    "conv_branch = GlobalMaxPooling1D()(conv_branch)\n",
    "\n",
    "# Apply global max pooling to the recurrent branches\n",
    "rnn_branch1 = GlobalMaxPooling1D()(rnn_branch1)\n",
    "rnn_branch2 = GlobalMaxPooling1D()(rnn_branch2)\n",
    "\n",
    "# Concatenate the outputs of all branches\n",
    "concatenated = concatenate([conv_branch, rnn_branch1, rnn_branch2])\n",
    "\n",
    "# Classification layers\n",
    "dense_layer = Dense(units=128, activation='relu')(concatenated)\n",
    "output_layer = Dense(units=nb_classes, activation='softmax')(dense_layer)\n",
    "\n",
    "# Create the model\n",
    "model1 = Model(inputs=input_layer, outputs=output_layer)\n",
    "\n",
    "# Compile the model\n",
    "model1.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 300, 1)]     0           []                               \n",
      "                                                                                                  \n",
      " conv1d (Conv1D)                (None, 298, 128)     512         ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " conv1d_1 (Conv1D)              (None, 296, 64)      24640       ['conv1d[0][0]']                 \n",
      "                                                                                                  \n",
      " max_pooling1d (MaxPooling1D)   (None, 148, 64)      0           ['conv1d_1[0][0]']               \n",
      "                                                                                                  \n",
      " lstm (LSTM)                    (None, 298, 64)      49408       ['conv1d[0][0]']                 \n",
      "                                                                                                  \n",
      " lstm_1 (LSTM)                  (None, 300, 64)      16896       ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " global_max_pooling1d (GlobalMa  (None, 64)          0           ['max_pooling1d[0][0]']          \n",
      " xPooling1D)                                                                                      \n",
      "                                                                                                  \n",
      " global_max_pooling1d_1 (Global  (None, 64)          0           ['lstm[0][0]']                   \n",
      " MaxPooling1D)                                                                                    \n",
      "                                                                                                  \n",
      " global_max_pooling1d_2 (Global  (None, 64)          0           ['lstm_1[0][0]']                 \n",
      " MaxPooling1D)                                                                                    \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 192)          0           ['global_max_pooling1d[0][0]',   \n",
      "                                                                  'global_max_pooling1d_1[0][0]', \n",
      "                                                                  'global_max_pooling1d_2[0][0]'] \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 128)          24704       ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 6)            774         ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 116,934\n",
      "Trainable params: 116,934\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pydot in c:\\users\\utku\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (1.4.2)\n",
      "Requirement already satisfied: pyparsing>=2.1.4 in c:\\users\\utku\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pydot) (3.0.8)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.0.1 -> 23.1.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install pydot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "You must install pydot (`pip install pydot`) for model_to_dot to work.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\UTKU\\OneDrive\\Desktop\\MS-Term4\\ProjectArbeit\\src\\main.ipynb Cell 26\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/UTKU/OneDrive/Desktop/MS-Term4/ProjectArbeit/src/main.ipynb#X65sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mIPython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdisplay\u001b[39;00m \u001b[39mimport\u001b[39;00m Image\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/UTKU/OneDrive/Desktop/MS-Term4/ProjectArbeit/src/main.ipynb#X65sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m# Convert the model to a graph in DOT format\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/UTKU/OneDrive/Desktop/MS-Term4/ProjectArbeit/src/main.ipynb#X65sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m dot_graph \u001b[39m=\u001b[39m model_to_dot(model1)\u001b[39m.\u001b[39mto_string()\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/UTKU/OneDrive/Desktop/MS-Term4/ProjectArbeit/src/main.ipynb#X65sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39m# Create a pydot graph from the DOT string\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/UTKU/OneDrive/Desktop/MS-Term4/ProjectArbeit/src/main.ipynb#X65sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m graph \u001b[39m=\u001b[39m pydot\u001b[39m.\u001b[39mgraph_from_dot_data(dot_graph)\n",
      "File \u001b[1;32mc:\\Users\\UTKU\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\utils\\vis_utils.py:138\u001b[0m, in \u001b[0;36mmodel_to_dot\u001b[1;34m(model, show_shapes, show_dtype, show_layer_names, rankdir, expand_nested, dpi, subgraph, layer_range, show_layer_activations)\u001b[0m\n\u001b[0;32m    135\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlayers\u001b[39;00m \u001b[39mimport\u001b[39;00m Wrapper\n\u001b[0;32m    137\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m check_pydot():\n\u001b[1;32m--> 138\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mImportError\u001b[39;00m(\n\u001b[0;32m    139\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mYou must install pydot (`pip install pydot`) for \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    140\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mmodel_to_dot to work.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    141\u001b[0m     )\n\u001b[0;32m    143\u001b[0m \u001b[39mif\u001b[39;00m subgraph:\n\u001b[0;32m    144\u001b[0m     dot \u001b[39m=\u001b[39m pydot\u001b[39m.\u001b[39mCluster(style\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mdashed\u001b[39m\u001b[39m\"\u001b[39m, graph_name\u001b[39m=\u001b[39mmodel\u001b[39m.\u001b[39mname)\n",
      "\u001b[1;31mImportError\u001b[0m: You must install pydot (`pip install pydot`) for model_to_dot to work."
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.utils import model_to_dot\n",
    "import pydot\n",
    "from IPython.display import Image\n",
    "# Convert the model to a graph in DOT format\n",
    "dot_graph = model_to_dot(model1).to_string()\n",
    "\n",
    "# Create a pydot graph from the DOT string\n",
    "graph = pydot.graph_from_dot_data(dot_graph)\n",
    "graph[0].write_png('model.png')\n",
    "\n",
    "# Display the image\n",
    "Image(filename='model.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model to work.\n"
     ]
    }
   ],
   "source": [
    "tf.keras.utils.plot_model(model1,'model.png', show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "86/86 [==============================] - 19s 186ms/step - loss: 1.2075 - accuracy: 0.3828 - val_loss: 1.1371 - val_accuracy: 0.6301\n",
      "Epoch 2/500\n",
      "86/86 [==============================] - 15s 176ms/step - loss: 1.1361 - accuracy: 0.4117 - val_loss: 1.1146 - val_accuracy: 0.6287\n",
      "Epoch 3/500\n",
      "86/86 [==============================] - 15s 174ms/step - loss: 1.0970 - accuracy: 0.4673 - val_loss: 1.0575 - val_accuracy: 0.4298\n",
      "Epoch 4/500\n",
      "86/86 [==============================] - 15s 172ms/step - loss: 0.9777 - accuracy: 0.5722 - val_loss: 0.8210 - val_accuracy: 0.6082\n",
      "Epoch 5/500\n",
      "86/86 [==============================] - 15s 171ms/step - loss: 0.7932 - accuracy: 0.5879 - val_loss: 0.7022 - val_accuracy: 0.6038\n",
      "Epoch 6/500\n",
      "86/86 [==============================] - 16s 191ms/step - loss: 0.9474 - accuracy: 0.5441 - val_loss: 0.8503 - val_accuracy: 0.6301\n",
      "Epoch 7/500\n",
      "86/86 [==============================] - 17s 194ms/step - loss: 0.8160 - accuracy: 0.5828 - val_loss: 0.7210 - val_accuracy: 0.6842\n",
      "Epoch 8/500\n",
      "86/86 [==============================] - 16s 188ms/step - loss: 0.7400 - accuracy: 0.6033 - val_loss: 0.6536 - val_accuracy: 0.6140\n",
      "Epoch 9/500\n",
      "86/86 [==============================] - 16s 187ms/step - loss: 0.6901 - accuracy: 0.6486 - val_loss: 0.6347 - val_accuracy: 0.6842\n",
      "Epoch 10/500\n",
      "86/86 [==============================] - 16s 184ms/step - loss: 0.6813 - accuracy: 0.6413 - val_loss: 0.6311 - val_accuracy: 0.6316\n",
      "Epoch 11/500\n",
      "86/86 [==============================] - 16s 185ms/step - loss: 0.6810 - accuracy: 0.6380 - val_loss: 0.6702 - val_accuracy: 0.6301\n",
      "Epoch 12/500\n",
      "86/86 [==============================] - 16s 186ms/step - loss: 0.6725 - accuracy: 0.6505 - val_loss: 0.6068 - val_accuracy: 0.6330\n",
      "Epoch 13/500\n",
      "86/86 [==============================] - 16s 186ms/step - loss: 0.6741 - accuracy: 0.6556 - val_loss: 0.7361 - val_accuracy: 0.6067\n",
      "Epoch 14/500\n",
      "86/86 [==============================] - 16s 183ms/step - loss: 0.6814 - accuracy: 0.6377 - val_loss: 0.6244 - val_accuracy: 0.6579\n",
      "Epoch 15/500\n",
      "86/86 [==============================] - 16s 182ms/step - loss: 0.6738 - accuracy: 0.6300 - val_loss: 0.6290 - val_accuracy: 0.6184\n",
      "Epoch 16/500\n",
      "86/86 [==============================] - 16s 181ms/step - loss: 0.6613 - accuracy: 0.6413 - val_loss: 0.5892 - val_accuracy: 0.7076\n",
      "Epoch 17/500\n",
      "86/86 [==============================] - 15s 180ms/step - loss: 0.6674 - accuracy: 0.6435 - val_loss: 0.6515 - val_accuracy: 0.7164\n",
      "Epoch 18/500\n",
      "86/86 [==============================] - 16s 180ms/step - loss: 0.6487 - accuracy: 0.6673 - val_loss: 0.6410 - val_accuracy: 0.6155\n",
      "Epoch 19/500\n",
      "86/86 [==============================] - 16s 183ms/step - loss: 0.6522 - accuracy: 0.6395 - val_loss: 0.5805 - val_accuracy: 0.6330\n",
      "Epoch 20/500\n",
      "86/86 [==============================] - 15s 179ms/step - loss: 0.6467 - accuracy: 0.6435 - val_loss: 0.5706 - val_accuracy: 0.6330\n",
      "Epoch 21/500\n",
      "86/86 [==============================] - 16s 182ms/step - loss: 0.6533 - accuracy: 0.6344 - val_loss: 0.5683 - val_accuracy: 0.7471\n",
      "Epoch 22/500\n",
      "86/86 [==============================] - 15s 178ms/step - loss: 0.6329 - accuracy: 0.6845 - val_loss: 0.5750 - val_accuracy: 0.6637\n",
      "Epoch 23/500\n",
      "86/86 [==============================] - 16s 181ms/step - loss: 0.6313 - accuracy: 0.6658 - val_loss: 0.5594 - val_accuracy: 0.8041\n",
      "Epoch 24/500\n",
      "86/86 [==============================] - 15s 180ms/step - loss: 0.6597 - accuracy: 0.6563 - val_loss: 0.5852 - val_accuracy: 0.6330\n",
      "Epoch 25/500\n",
      "86/86 [==============================] - 15s 180ms/step - loss: 0.6382 - accuracy: 0.6589 - val_loss: 0.5859 - val_accuracy: 0.6535\n",
      "Epoch 26/500\n",
      "86/86 [==============================] - 16s 181ms/step - loss: 0.6371 - accuracy: 0.6629 - val_loss: 0.5831 - val_accuracy: 0.6535\n",
      "Epoch 27/500\n",
      "86/86 [==============================] - 15s 179ms/step - loss: 0.6471 - accuracy: 0.6534 - val_loss: 0.6911 - val_accuracy: 0.6140\n",
      "Epoch 28/500\n",
      "86/86 [==============================] - 16s 181ms/step - loss: 0.6452 - accuracy: 0.6684 - val_loss: 0.5618 - val_accuracy: 0.9327\n",
      "Epoch 29/500\n",
      "86/86 [==============================] - 16s 181ms/step - loss: 0.6523 - accuracy: 0.6428 - val_loss: 0.6658 - val_accuracy: 0.6330\n",
      "Epoch 30/500\n",
      "86/86 [==============================] - 15s 179ms/step - loss: 0.6515 - accuracy: 0.6406 - val_loss: 0.5887 - val_accuracy: 0.6594\n",
      "Epoch 31/500\n",
      "86/86 [==============================] - 16s 180ms/step - loss: 0.6247 - accuracy: 0.7049 - val_loss: 0.5613 - val_accuracy: 0.9284\n",
      "Epoch 32/500\n",
      "86/86 [==============================] - 16s 182ms/step - loss: 0.6348 - accuracy: 0.6475 - val_loss: 0.5599 - val_accuracy: 0.7281\n",
      "Epoch 33/500\n",
      "86/86 [==============================] - 15s 180ms/step - loss: 0.6392 - accuracy: 0.6841 - val_loss: 0.6944 - val_accuracy: 0.6330\n",
      "Epoch 34/500\n",
      "86/86 [==============================] - 15s 179ms/step - loss: 0.6429 - accuracy: 0.6585 - val_loss: 0.5835 - val_accuracy: 0.6330\n",
      "Epoch 35/500\n",
      "86/86 [==============================] - 16s 181ms/step - loss: 0.6315 - accuracy: 0.6779 - val_loss: 0.5591 - val_accuracy: 0.6330\n",
      "Epoch 36/500\n",
      "86/86 [==============================] - 16s 186ms/step - loss: 0.6241 - accuracy: 0.6552 - val_loss: 0.5634 - val_accuracy: 0.6330\n",
      "Epoch 37/500\n",
      "86/86 [==============================] - 15s 179ms/step - loss: 0.6254 - accuracy: 0.6874 - val_loss: 0.5895 - val_accuracy: 0.6535\n",
      "Epoch 38/500\n",
      "86/86 [==============================] - 16s 182ms/step - loss: 0.6082 - accuracy: 0.7053 - val_loss: 0.5657 - val_accuracy: 0.6608\n",
      "Epoch 39/500\n",
      "86/86 [==============================] - 16s 180ms/step - loss: 0.6052 - accuracy: 0.7152 - val_loss: 0.6175 - val_accuracy: 0.6404\n",
      "Epoch 40/500\n",
      "86/86 [==============================] - 15s 180ms/step - loss: 0.6203 - accuracy: 0.6859 - val_loss: 0.5343 - val_accuracy: 0.8874\n",
      "Epoch 41/500\n",
      "86/86 [==============================] - 15s 179ms/step - loss: 0.6136 - accuracy: 0.6998 - val_loss: 0.5484 - val_accuracy: 0.6637\n",
      "Epoch 42/500\n",
      "86/86 [==============================] - 16s 185ms/step - loss: 0.6067 - accuracy: 0.7086 - val_loss: 0.5478 - val_accuracy: 0.7617\n",
      "Epoch 43/500\n",
      "86/86 [==============================] - 16s 180ms/step - loss: 0.6228 - accuracy: 0.6892 - val_loss: 0.6203 - val_accuracy: 0.6330\n",
      "Epoch 44/500\n",
      "86/86 [==============================] - 16s 183ms/step - loss: 0.5938 - accuracy: 0.7261 - val_loss: 0.5419 - val_accuracy: 0.6959\n",
      "Epoch 45/500\n",
      "86/86 [==============================] - 16s 181ms/step - loss: 0.5901 - accuracy: 0.7148 - val_loss: 0.5209 - val_accuracy: 0.9240\n",
      "Epoch 46/500\n",
      "86/86 [==============================] - 16s 180ms/step - loss: 0.6326 - accuracy: 0.6954 - val_loss: 0.5250 - val_accuracy: 0.9503\n",
      "Epoch 47/500\n",
      "86/86 [==============================] - 15s 180ms/step - loss: 0.5857 - accuracy: 0.7484 - val_loss: 0.5221 - val_accuracy: 0.8187\n",
      "Epoch 48/500\n",
      "86/86 [==============================] - 16s 181ms/step - loss: 0.6014 - accuracy: 0.7155 - val_loss: 0.5292 - val_accuracy: 0.6944\n",
      "Epoch 49/500\n",
      "86/86 [==============================] - 15s 180ms/step - loss: 0.5894 - accuracy: 0.7349 - val_loss: 0.5198 - val_accuracy: 0.8260\n",
      "Epoch 50/500\n",
      "86/86 [==============================] - 15s 180ms/step - loss: 0.5685 - accuracy: 0.7532 - val_loss: 0.5250 - val_accuracy: 0.6827\n",
      "Epoch 51/500\n",
      "86/86 [==============================] - 15s 180ms/step - loss: 0.5710 - accuracy: 0.7408 - val_loss: 0.5086 - val_accuracy: 0.7485\n",
      "Epoch 52/500\n",
      "86/86 [==============================] - 16s 181ms/step - loss: 0.5612 - accuracy: 0.7715 - val_loss: 0.4826 - val_accuracy: 0.9708\n",
      "Epoch 53/500\n",
      "86/86 [==============================] - 15s 180ms/step - loss: 0.5630 - accuracy: 0.7664 - val_loss: 0.5635 - val_accuracy: 0.6696\n",
      "Epoch 54/500\n",
      "86/86 [==============================] - 15s 173ms/step - loss: 0.6095 - accuracy: 0.6958 - val_loss: 0.5363 - val_accuracy: 0.6462\n",
      "Epoch 55/500\n",
      "86/86 [==============================] - 15s 179ms/step - loss: 0.6007 - accuracy: 0.7115 - val_loss: 0.5280 - val_accuracy: 0.6608\n",
      "Epoch 56/500\n",
      "86/86 [==============================] - 15s 179ms/step - loss: 0.5701 - accuracy: 0.7550 - val_loss: 0.4908 - val_accuracy: 0.9020\n",
      "Epoch 57/500\n",
      "86/86 [==============================] - 15s 178ms/step - loss: 0.5528 - accuracy: 0.7803 - val_loss: 0.5264 - val_accuracy: 0.6827\n",
      "Epoch 58/500\n",
      "86/86 [==============================] - 15s 179ms/step - loss: 0.5437 - accuracy: 0.7561 - val_loss: 0.4966 - val_accuracy: 0.8070\n",
      "Epoch 59/500\n",
      "86/86 [==============================] - 15s 179ms/step - loss: 0.5502 - accuracy: 0.7623 - val_loss: 0.4728 - val_accuracy: 0.9298\n",
      "Epoch 60/500\n",
      "86/86 [==============================] - 15s 178ms/step - loss: 0.5298 - accuracy: 0.8011 - val_loss: 0.4602 - val_accuracy: 0.9342\n",
      "Epoch 61/500\n",
      "86/86 [==============================] - 15s 178ms/step - loss: 0.5382 - accuracy: 0.7920 - val_loss: 0.4522 - val_accuracy: 0.9649\n",
      "Epoch 62/500\n",
      "86/86 [==============================] - 16s 182ms/step - loss: 0.5468 - accuracy: 0.7762 - val_loss: 0.4526 - val_accuracy: 0.9693\n",
      "Epoch 63/500\n",
      "86/86 [==============================] - 16s 181ms/step - loss: 0.5527 - accuracy: 0.7477 - val_loss: 0.4501 - val_accuracy: 0.9444\n",
      "Epoch 64/500\n",
      "86/86 [==============================] - 15s 180ms/step - loss: 0.5287 - accuracy: 0.7792 - val_loss: 0.5691 - val_accuracy: 0.7719\n",
      "Epoch 65/500\n",
      "86/86 [==============================] - 16s 181ms/step - loss: 0.5293 - accuracy: 0.7989 - val_loss: 0.4700 - val_accuracy: 0.8348\n",
      "Epoch 66/500\n",
      "86/86 [==============================] - 16s 183ms/step - loss: 0.5133 - accuracy: 0.7912 - val_loss: 0.5274 - val_accuracy: 0.8041\n",
      "Epoch 67/500\n",
      "86/86 [==============================] - 16s 185ms/step - loss: 0.5084 - accuracy: 0.8113 - val_loss: 0.6013 - val_accuracy: 0.7427\n",
      "Epoch 68/500\n",
      "86/86 [==============================] - 16s 181ms/step - loss: 0.5094 - accuracy: 0.8146 - val_loss: 0.4350 - val_accuracy: 0.7734\n",
      "Epoch 69/500\n",
      "86/86 [==============================] - 15s 179ms/step - loss: 0.4976 - accuracy: 0.7784 - val_loss: 0.4280 - val_accuracy: 0.7924\n",
      "Epoch 70/500\n",
      "86/86 [==============================] - 16s 181ms/step - loss: 0.4786 - accuracy: 0.8124 - val_loss: 0.4710 - val_accuracy: 0.8158\n",
      "Epoch 71/500\n",
      "86/86 [==============================] - 16s 181ms/step - loss: 0.4661 - accuracy: 0.8241 - val_loss: 0.4043 - val_accuracy: 0.8567\n",
      "Epoch 72/500\n",
      "86/86 [==============================] - 16s 182ms/step - loss: 0.4721 - accuracy: 0.8080 - val_loss: 0.3901 - val_accuracy: 0.9108\n",
      "Epoch 73/500\n",
      "86/86 [==============================] - 16s 182ms/step - loss: 0.4321 - accuracy: 0.8746 - val_loss: 0.4408 - val_accuracy: 0.7573\n",
      "Epoch 74/500\n",
      "86/86 [==============================] - 16s 181ms/step - loss: 0.4404 - accuracy: 0.8314 - val_loss: 0.3598 - val_accuracy: 0.9620\n",
      "Epoch 75/500\n",
      "86/86 [==============================] - 16s 182ms/step - loss: 0.4202 - accuracy: 0.8494 - val_loss: 0.3956 - val_accuracy: 0.8173\n",
      "Epoch 76/500\n",
      "86/86 [==============================] - 15s 179ms/step - loss: 0.4907 - accuracy: 0.8176 - val_loss: 0.3450 - val_accuracy: 0.9781\n",
      "Epoch 77/500\n",
      "86/86 [==============================] - 15s 179ms/step - loss: 0.4622 - accuracy: 0.8512 - val_loss: 0.5007 - val_accuracy: 0.7968\n",
      "Epoch 78/500\n",
      "86/86 [==============================] - 16s 182ms/step - loss: 0.4047 - accuracy: 0.8654 - val_loss: 0.3244 - val_accuracy: 0.9474\n",
      "Epoch 79/500\n",
      "86/86 [==============================] - 16s 183ms/step - loss: 0.3660 - accuracy: 0.9005 - val_loss: 0.3929 - val_accuracy: 0.8260\n",
      "Epoch 80/500\n",
      "86/86 [==============================] - 15s 177ms/step - loss: 0.4047 - accuracy: 0.8622 - val_loss: 0.9654 - val_accuracy: 0.6330\n",
      "Epoch 81/500\n",
      "86/86 [==============================] - 15s 176ms/step - loss: 0.5310 - accuracy: 0.7916 - val_loss: 0.6556 - val_accuracy: 0.7909\n",
      "Epoch 82/500\n",
      "86/86 [==============================] - 16s 183ms/step - loss: 0.4088 - accuracy: 0.8644 - val_loss: 0.3205 - val_accuracy: 0.9766\n",
      "Epoch 83/500\n",
      "86/86 [==============================] - 16s 183ms/step - loss: 0.3805 - accuracy: 0.8845 - val_loss: 0.4891 - val_accuracy: 0.7617\n",
      "Epoch 84/500\n",
      "86/86 [==============================] - 16s 181ms/step - loss: 0.3944 - accuracy: 0.8771 - val_loss: 0.4175 - val_accuracy: 0.8070\n",
      "Epoch 85/500\n",
      "86/86 [==============================] - 16s 181ms/step - loss: 0.3652 - accuracy: 0.8815 - val_loss: 0.2917 - val_accuracy: 0.9825\n",
      "Epoch 86/500\n",
      "86/86 [==============================] - 16s 181ms/step - loss: 0.4003 - accuracy: 0.8731 - val_loss: 0.4716 - val_accuracy: 0.7953\n",
      "Epoch 87/500\n",
      "86/86 [==============================] - 16s 181ms/step - loss: 0.3889 - accuracy: 0.8607 - val_loss: 0.4993 - val_accuracy: 0.7953\n",
      "Epoch 88/500\n",
      "86/86 [==============================] - 16s 182ms/step - loss: 0.3341 - accuracy: 0.9112 - val_loss: 0.2602 - val_accuracy: 0.9810\n",
      "Epoch 89/500\n",
      "86/86 [==============================] - 16s 183ms/step - loss: 0.3077 - accuracy: 0.9305 - val_loss: 0.2526 - val_accuracy: 0.9751\n",
      "Epoch 90/500\n",
      "86/86 [==============================] - 16s 182ms/step - loss: 0.3125 - accuracy: 0.9090 - val_loss: 0.2876 - val_accuracy: 0.9254\n",
      "Epoch 91/500\n",
      "86/86 [==============================] - 16s 182ms/step - loss: 0.3110 - accuracy: 0.8991 - val_loss: 0.2467 - val_accuracy: 0.9766\n",
      "Epoch 92/500\n",
      "86/86 [==============================] - 16s 185ms/step - loss: 0.2781 - accuracy: 0.9342 - val_loss: 0.2251 - val_accuracy: 0.9781\n",
      "Epoch 93/500\n",
      "86/86 [==============================] - 16s 183ms/step - loss: 0.2870 - accuracy: 0.9196 - val_loss: 0.2094 - val_accuracy: 0.9781\n",
      "Epoch 94/500\n",
      "86/86 [==============================] - 16s 182ms/step - loss: 0.3337 - accuracy: 0.8713 - val_loss: 0.4133 - val_accuracy: 0.7778\n",
      "Epoch 95/500\n",
      "86/86 [==============================] - 16s 182ms/step - loss: 0.2754 - accuracy: 0.9313 - val_loss: 0.2743 - val_accuracy: 0.8582\n",
      "Epoch 96/500\n",
      "86/86 [==============================] - 16s 184ms/step - loss: 0.2746 - accuracy: 0.9192 - val_loss: 0.1792 - val_accuracy: 0.9854\n",
      "Epoch 97/500\n",
      "86/86 [==============================] - 16s 184ms/step - loss: 0.2523 - accuracy: 0.9353 - val_loss: 0.1712 - val_accuracy: 0.9825\n",
      "Epoch 98/500\n",
      "86/86 [==============================] - 16s 183ms/step - loss: 0.2566 - accuracy: 0.9163 - val_loss: 0.2152 - val_accuracy: 0.9576\n",
      "Epoch 99/500\n",
      "86/86 [==============================] - 16s 181ms/step - loss: 0.3442 - accuracy: 0.8958 - val_loss: 0.2187 - val_accuracy: 0.9708\n",
      "Epoch 100/500\n",
      "86/86 [==============================] - 16s 184ms/step - loss: 0.2323 - accuracy: 0.9400 - val_loss: 0.1627 - val_accuracy: 0.9825\n",
      "Epoch 101/500\n",
      "86/86 [==============================] - 16s 181ms/step - loss: 0.2805 - accuracy: 0.9152 - val_loss: 0.2491 - val_accuracy: 0.8787\n",
      "Epoch 102/500\n",
      "86/86 [==============================] - 16s 184ms/step - loss: 0.2650 - accuracy: 0.9060 - val_loss: 0.2116 - val_accuracy: 0.9737\n",
      "Epoch 103/500\n",
      "86/86 [==============================] - 15s 178ms/step - loss: 0.2456 - accuracy: 0.9214 - val_loss: 0.1743 - val_accuracy: 0.9751\n",
      "Epoch 104/500\n",
      "86/86 [==============================] - 16s 185ms/step - loss: 0.3147 - accuracy: 0.8852 - val_loss: 0.2169 - val_accuracy: 0.9722\n",
      "Epoch 105/500\n",
      "86/86 [==============================] - 16s 185ms/step - loss: 0.2415 - accuracy: 0.9247 - val_loss: 0.2159 - val_accuracy: 0.9708\n",
      "Epoch 106/500\n",
      "86/86 [==============================] - 16s 191ms/step - loss: 0.2023 - accuracy: 0.9547 - val_loss: 0.2136 - val_accuracy: 0.9518\n",
      "Epoch 107/500\n",
      "86/86 [==============================] - 16s 186ms/step - loss: 0.1862 - accuracy: 0.9612 - val_loss: 0.3838 - val_accuracy: 0.8114\n",
      "Epoch 108/500\n",
      "86/86 [==============================] - 16s 187ms/step - loss: 0.4527 - accuracy: 0.8607 - val_loss: 0.2755 - val_accuracy: 0.8377\n",
      "Epoch 109/500\n",
      "86/86 [==============================] - 16s 185ms/step - loss: 0.2210 - accuracy: 0.9411 - val_loss: 0.2814 - val_accuracy: 0.8304\n",
      "Epoch 110/500\n",
      "86/86 [==============================] - 16s 184ms/step - loss: 0.2132 - accuracy: 0.9455 - val_loss: 0.1713 - val_accuracy: 0.9737\n",
      "Epoch 111/500\n",
      "86/86 [==============================] - 16s 183ms/step - loss: 0.2595 - accuracy: 0.9287 - val_loss: 0.2510 - val_accuracy: 0.9415\n",
      "Epoch 112/500\n",
      "86/86 [==============================] - 16s 183ms/step - loss: 0.2247 - accuracy: 0.9393 - val_loss: 0.1937 - val_accuracy: 0.9751\n",
      "Epoch 113/500\n",
      "86/86 [==============================] - 16s 185ms/step - loss: 0.1823 - accuracy: 0.9664 - val_loss: 0.2201 - val_accuracy: 0.8962\n",
      "Epoch 114/500\n",
      "86/86 [==============================] - 16s 183ms/step - loss: 0.1699 - accuracy: 0.9711 - val_loss: 0.1576 - val_accuracy: 0.9737\n",
      "Epoch 115/500\n",
      "86/86 [==============================] - 16s 183ms/step - loss: 0.1664 - accuracy: 0.9700 - val_loss: 0.1254 - val_accuracy: 0.9825\n",
      "Epoch 116/500\n",
      "86/86 [==============================] - 16s 183ms/step - loss: 0.2044 - accuracy: 0.9404 - val_loss: 0.2003 - val_accuracy: 0.9108\n",
      "Epoch 117/500\n",
      "86/86 [==============================] - 16s 183ms/step - loss: 0.1769 - accuracy: 0.9488 - val_loss: 0.1586 - val_accuracy: 0.9664\n",
      "Epoch 118/500\n",
      "86/86 [==============================] - 16s 182ms/step - loss: 0.1730 - accuracy: 0.9547 - val_loss: 0.1118 - val_accuracy: 0.9839\n",
      "Epoch 119/500\n",
      "86/86 [==============================] - 16s 186ms/step - loss: 0.1592 - accuracy: 0.9590 - val_loss: 1.3319 - val_accuracy: 0.7763\n",
      "Epoch 120/500\n",
      "86/86 [==============================] - 16s 183ms/step - loss: 0.2005 - accuracy: 0.9386 - val_loss: 0.1446 - val_accuracy: 0.9737\n",
      "Epoch 121/500\n",
      "86/86 [==============================] - 16s 184ms/step - loss: 0.1448 - accuracy: 0.9693 - val_loss: 0.1482 - val_accuracy: 0.9664\n",
      "Epoch 122/500\n",
      "86/86 [==============================] - 16s 184ms/step - loss: 0.2353 - accuracy: 0.9210 - val_loss: 0.1263 - val_accuracy: 0.9810\n",
      "Epoch 123/500\n",
      "86/86 [==============================] - 16s 184ms/step - loss: 0.1556 - accuracy: 0.9580 - val_loss: 0.1974 - val_accuracy: 0.9167\n",
      "Epoch 124/500\n",
      "86/86 [==============================] - 16s 185ms/step - loss: 0.3052 - accuracy: 0.8954 - val_loss: 1.3048 - val_accuracy: 0.7705\n",
      "Epoch 125/500\n",
      "86/86 [==============================] - 16s 192ms/step - loss: 0.2428 - accuracy: 0.9229 - val_loss: 0.1034 - val_accuracy: 0.9825\n",
      "Epoch 126/500\n",
      "86/86 [==============================] - 16s 186ms/step - loss: 0.1562 - accuracy: 0.9532 - val_loss: 0.1101 - val_accuracy: 0.9825\n",
      "Epoch 127/500\n",
      "86/86 [==============================] - 16s 187ms/step - loss: 0.1719 - accuracy: 0.9463 - val_loss: 0.0960 - val_accuracy: 0.9839\n",
      "Epoch 128/500\n",
      "86/86 [==============================] - 16s 180ms/step - loss: 0.1201 - accuracy: 0.9792 - val_loss: 0.0944 - val_accuracy: 0.9854\n",
      "Epoch 129/500\n",
      "86/86 [==============================] - 15s 180ms/step - loss: 0.2027 - accuracy: 0.9316 - val_loss: 0.2918 - val_accuracy: 0.9167\n",
      "Epoch 130/500\n",
      "86/86 [==============================] - 15s 179ms/step - loss: 0.1264 - accuracy: 0.9733 - val_loss: 0.0840 - val_accuracy: 0.9825\n",
      "Epoch 131/500\n",
      "86/86 [==============================] - 15s 179ms/step - loss: 0.1961 - accuracy: 0.9320 - val_loss: 0.0995 - val_accuracy: 0.9810\n",
      "Epoch 132/500\n",
      "86/86 [==============================] - 15s 179ms/step - loss: 0.1552 - accuracy: 0.9495 - val_loss: 0.0904 - val_accuracy: 0.9825\n",
      "Epoch 133/500\n",
      "86/86 [==============================] - 15s 179ms/step - loss: 0.0962 - accuracy: 0.9806 - val_loss: 0.0775 - val_accuracy: 0.9839\n",
      "Epoch 134/500\n",
      "86/86 [==============================] - 15s 179ms/step - loss: 0.1949 - accuracy: 0.9276 - val_loss: 0.2825 - val_accuracy: 0.8406\n",
      "Epoch 135/500\n",
      "86/86 [==============================] - 15s 179ms/step - loss: 0.1571 - accuracy: 0.9433 - val_loss: 0.7485 - val_accuracy: 0.7895\n",
      "Epoch 136/500\n",
      "86/86 [==============================] - 15s 177ms/step - loss: 0.1717 - accuracy: 0.9492 - val_loss: 0.0991 - val_accuracy: 0.9825\n",
      "Epoch 137/500\n",
      "86/86 [==============================] - 17s 193ms/step - loss: 0.1625 - accuracy: 0.9543 - val_loss: 0.6327 - val_accuracy: 0.7822\n",
      "Epoch 138/500\n",
      "86/86 [==============================] - 16s 186ms/step - loss: 0.1444 - accuracy: 0.9634 - val_loss: 0.0876 - val_accuracy: 0.9825\n",
      "Epoch 139/500\n",
      "86/86 [==============================] - 16s 182ms/step - loss: 0.1454 - accuracy: 0.9543 - val_loss: 0.1383 - val_accuracy: 0.9576\n",
      "Epoch 140/500\n",
      "86/86 [==============================] - 16s 181ms/step - loss: 0.1012 - accuracy: 0.9762 - val_loss: 0.0760 - val_accuracy: 0.9868\n",
      "Epoch 141/500\n",
      "86/86 [==============================] - 16s 183ms/step - loss: 0.1981 - accuracy: 0.9324 - val_loss: 0.1665 - val_accuracy: 0.9415\n",
      "Epoch 142/500\n",
      "86/86 [==============================] - 16s 181ms/step - loss: 0.1628 - accuracy: 0.9503 - val_loss: 0.0807 - val_accuracy: 0.9839\n",
      "Epoch 143/500\n",
      "86/86 [==============================] - 16s 185ms/step - loss: 0.0981 - accuracy: 0.9762 - val_loss: 0.0735 - val_accuracy: 0.9854\n",
      "Epoch 144/500\n",
      "86/86 [==============================] - 16s 184ms/step - loss: 0.1076 - accuracy: 0.9704 - val_loss: 0.0928 - val_accuracy: 0.9825\n",
      "Epoch 145/500\n",
      "86/86 [==============================] - 16s 182ms/step - loss: 0.0964 - accuracy: 0.9770 - val_loss: 0.4149 - val_accuracy: 0.7792\n",
      "Epoch 146/500\n",
      "86/86 [==============================] - 16s 182ms/step - loss: 0.2482 - accuracy: 0.9298 - val_loss: 0.0813 - val_accuracy: 0.9825\n",
      "Epoch 147/500\n",
      "86/86 [==============================] - 16s 181ms/step - loss: 0.0827 - accuracy: 0.9824 - val_loss: 0.0740 - val_accuracy: 0.9839\n",
      "Epoch 148/500\n",
      "86/86 [==============================] - 16s 182ms/step - loss: 0.0855 - accuracy: 0.9803 - val_loss: 0.0702 - val_accuracy: 0.9825\n",
      "Epoch 149/500\n",
      "86/86 [==============================] - 16s 181ms/step - loss: 0.2596 - accuracy: 0.9335 - val_loss: 1.5938 - val_accuracy: 0.7792\n",
      "Epoch 150/500\n",
      "86/86 [==============================] - 16s 186ms/step - loss: 0.3477 - accuracy: 0.8874 - val_loss: 0.1967 - val_accuracy: 0.9079\n",
      "Epoch 151/500\n",
      "86/86 [==============================] - 16s 183ms/step - loss: 0.1413 - accuracy: 0.9645 - val_loss: 0.1163 - val_accuracy: 0.9810\n",
      "Epoch 152/500\n",
      "86/86 [==============================] - 15s 180ms/step - loss: 0.1189 - accuracy: 0.9678 - val_loss: 0.1068 - val_accuracy: 0.9766\n",
      "Epoch 153/500\n",
      "86/86 [==============================] - 16s 181ms/step - loss: 0.1341 - accuracy: 0.9569 - val_loss: 0.0979 - val_accuracy: 0.9854\n",
      "Epoch 154/500\n",
      "86/86 [==============================] - 16s 184ms/step - loss: 0.0964 - accuracy: 0.9795 - val_loss: 0.0756 - val_accuracy: 0.9839\n",
      "Epoch 155/500\n",
      "86/86 [==============================] - 16s 190ms/step - loss: 0.1503 - accuracy: 0.9495 - val_loss: 0.1017 - val_accuracy: 0.9810\n",
      "Epoch 156/500\n",
      "86/86 [==============================] - 16s 185ms/step - loss: 0.1300 - accuracy: 0.9587 - val_loss: 0.0714 - val_accuracy: 0.9854\n",
      "Epoch 157/500\n",
      "86/86 [==============================] - 16s 187ms/step - loss: 0.0978 - accuracy: 0.9777 - val_loss: 0.0830 - val_accuracy: 0.9810\n",
      "Epoch 158/500\n",
      "86/86 [==============================] - 16s 185ms/step - loss: 0.2461 - accuracy: 0.9239 - val_loss: 0.7386 - val_accuracy: 0.7953\n",
      "Epoch 159/500\n",
      "86/86 [==============================] - 16s 186ms/step - loss: 0.2362 - accuracy: 0.9218 - val_loss: 0.0811 - val_accuracy: 0.9839\n",
      "Epoch 160/500\n",
      "86/86 [==============================] - 16s 181ms/step - loss: 0.1462 - accuracy: 0.9466 - val_loss: 0.3485 - val_accuracy: 0.8041\n",
      "Epoch 161/500\n",
      "86/86 [==============================] - 16s 183ms/step - loss: 0.1945 - accuracy: 0.9272 - val_loss: 0.0910 - val_accuracy: 0.9839\n",
      "Epoch 162/500\n",
      "86/86 [==============================] - 16s 182ms/step - loss: 0.0948 - accuracy: 0.9766 - val_loss: 0.0702 - val_accuracy: 0.9868\n",
      "Epoch 163/500\n",
      "86/86 [==============================] - 15s 179ms/step - loss: 0.1148 - accuracy: 0.9638 - val_loss: 0.0648 - val_accuracy: 0.9854\n",
      "Epoch 164/500\n",
      "86/86 [==============================] - 16s 181ms/step - loss: 0.1097 - accuracy: 0.9700 - val_loss: 0.0699 - val_accuracy: 0.9839\n",
      "Epoch 165/500\n",
      "86/86 [==============================] - 16s 187ms/step - loss: 0.0792 - accuracy: 0.9803 - val_loss: 0.0674 - val_accuracy: 0.9839\n",
      "Epoch 166/500\n",
      "86/86 [==============================] - 15s 180ms/step - loss: 0.0852 - accuracy: 0.9762 - val_loss: 0.0516 - val_accuracy: 0.9868\n",
      "Epoch 167/500\n",
      "86/86 [==============================] - 17s 195ms/step - loss: 0.1554 - accuracy: 0.9484 - val_loss: 0.0874 - val_accuracy: 0.9766\n",
      "Epoch 168/500\n",
      "86/86 [==============================] - 18s 208ms/step - loss: 0.0951 - accuracy: 0.9704 - val_loss: 0.3125 - val_accuracy: 0.8450\n",
      "Epoch 169/500\n",
      "86/86 [==============================] - 16s 191ms/step - loss: 0.1501 - accuracy: 0.9506 - val_loss: 0.0976 - val_accuracy: 0.9810\n",
      "Epoch 170/500\n",
      "86/86 [==============================] - 16s 192ms/step - loss: 0.0832 - accuracy: 0.9803 - val_loss: 0.0627 - val_accuracy: 0.9854\n",
      "Epoch 171/500\n",
      "86/86 [==============================] - 17s 193ms/step - loss: 0.1124 - accuracy: 0.9649 - val_loss: 0.0833 - val_accuracy: 0.9825\n",
      "Epoch 172/500\n",
      "86/86 [==============================] - 17s 192ms/step - loss: 0.3511 - accuracy: 0.9320 - val_loss: 1.4090 - val_accuracy: 0.7836\n",
      "Epoch 173/500\n",
      "86/86 [==============================] - 16s 191ms/step - loss: 0.3514 - accuracy: 0.8782 - val_loss: 0.1834 - val_accuracy: 0.9649\n",
      "Epoch 174/500\n",
      "86/86 [==============================] - 17s 192ms/step - loss: 0.2000 - accuracy: 0.9488 - val_loss: 0.1330 - val_accuracy: 0.9722\n",
      "Epoch 175/500\n",
      "86/86 [==============================] - 16s 191ms/step - loss: 0.1394 - accuracy: 0.9605 - val_loss: 0.1158 - val_accuracy: 0.9722\n",
      "Epoch 176/500\n",
      "86/86 [==============================] - 17s 194ms/step - loss: 0.0938 - accuracy: 0.9810 - val_loss: 0.0726 - val_accuracy: 0.9854\n",
      "Epoch 177/500\n",
      "86/86 [==============================] - 17s 193ms/step - loss: 0.1330 - accuracy: 0.9503 - val_loss: 0.3179 - val_accuracy: 0.8260\n",
      "Epoch 178/500\n",
      "86/86 [==============================] - 16s 191ms/step - loss: 0.0925 - accuracy: 0.9755 - val_loss: 0.0723 - val_accuracy: 0.9868\n",
      "Epoch 179/500\n",
      "86/86 [==============================] - 16s 190ms/step - loss: 0.0732 - accuracy: 0.9843 - val_loss: 0.0577 - val_accuracy: 0.9898\n",
      "Epoch 180/500\n",
      "86/86 [==============================] - 17s 193ms/step - loss: 0.1585 - accuracy: 0.9477 - val_loss: 0.2813 - val_accuracy: 0.8538\n",
      "Epoch 181/500\n",
      "86/86 [==============================] - 17s 194ms/step - loss: 0.1422 - accuracy: 0.9484 - val_loss: 0.0660 - val_accuracy: 0.9868\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "batch_size = 32\n",
    "nb_epochs = 500\n",
    "\n",
    "# Define early stopping callback\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True)\n",
    "\n",
    "# Compile the model\n",
    "#model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Train the model with optimizations\n",
    "hist = model1.fit(X_train, y_train, batch_size=batch_size, epochs=nb_epochs,\n",
    "                 validation_data=(X_test, y_test), callbacks=[early_stopping])\n",
    "\n",
    "model1.save('model_parallel_azad.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "nb_epochs = 750\n",
    "\n",
    "hist = model1.fit(X_train, y_train, batch_size=batch_size, epochs=nb_epochs, validation_data=(X_test, y_test))\n",
    "\n",
    "model1.save('model_parallel3.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1.save('model_parallel3.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_values = pd.read_pickle('test_data_values')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test_labels = pd.read_pickle('test_labels_300_10')\n",
    "test_labels.to_excel(\"test_label_300_10.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_values = pd.read_pickle('test_data_values_300_10')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_values.to_excel(\"test_300_10.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_values = test_values.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_values = test_values.values\n",
    "test_values = test_values.reshape((test_values.shape[0], test_values.shape[1], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2046, 300, 1)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64/64 [==============================] - 4s 45ms/step\n"
     ]
    }
   ],
   "source": [
    "predictions = model1.predict(test_values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[7.1996611e-01, 2.7991813e-01, 2.8486701e-32, 3.9707193e-06,\n",
       "        9.1516260e-05, 2.0316718e-05],\n",
       "       [1.2887692e-03, 9.9853849e-01, 7.2795458e-36, 1.9557078e-07,\n",
       "        1.7170074e-04, 8.2074331e-07],\n",
       "       [2.8859754e-06, 9.9988651e-01, 0.0000000e+00, 6.7296742e-09,\n",
       "        1.1062447e-04, 2.1289589e-08],\n",
       "       ...,\n",
       "       [3.4590322e-09, 2.1541282e-04, 2.3986621e-01, 4.6660103e-02,\n",
       "        2.8359180e-08, 7.1325821e-01],\n",
       "       [8.9666915e-14, 3.9594557e-08, 9.9522829e-01, 3.4261236e-04,\n",
       "        9.1347320e-12, 4.4290787e-03],\n",
       "       [1.0243467e-08, 3.8492185e-04, 3.8140662e-02, 2.6790526e-02,\n",
       "        4.7776886e-08, 9.3468386e-01]], dtype=float32)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                Prediction           Prediction2\n",
      "0                     Bias                 Drift\n",
      "1                    Drift                  Bias\n",
      "2                    Drift              Outliers\n",
      "3                    Drift               NoFault\n",
      "4                    Drift               NoFault\n",
      "...                    ...                   ...\n",
      "2041  Precisiondegradation               NoFault\n",
      "2042  Precisiondegradation                  Gain\n",
      "2043  Precisiondegradation                  Gain\n",
      "2044                  Gain  Precisiondegradation\n",
      "2045  Precisiondegradation                  Gain\n",
      "\n",
      "[2046 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "prediction_list = []\n",
    "\n",
    "threshold = 0.0001\n",
    "\n",
    "# Iterate over each row of predictions\n",
    "for row in predictions:\n",
    "    # Sort the probabilities in descending order and get the indices\n",
    "    sorted_indices = np.argsort(row)[::-1]\n",
    "\n",
    "    # Check if the highest prediction is above the threshold\n",
    "    if row[sorted_indices[0]] >= threshold:\n",
    "        prediction1 = sorted_indices[0]\n",
    "    else:\n",
    "        prediction1 = 3\n",
    "\n",
    "    # Check if the second highest prediction is above the threshold\n",
    "    if row[sorted_indices[1]] >= threshold:\n",
    "        prediction2 = sorted_indices[1]\n",
    "    else:\n",
    "        prediction2 = 3\n",
    "\n",
    "    # Add the predictions to the list\n",
    "    prediction_list.append([prediction1, prediction2])\n",
    "\n",
    "# Create the DataFrame from the list\n",
    "#df = pd.DataFrame(prediction_list, columns=['prediction1', 'prediction2'])\n",
    "prediction_list = np.array(prediction_list)\n",
    "labels = ['Bias', 'Drift', 'Gain', 'NoFault', 'Outliers', 'Precisiondegradation']\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(labels)\n",
    "predicted_labels_1 = label_encoder.inverse_transform(prediction_list[:,0])\n",
    "predicted_labels_2 = label_encoder.inverse_transform(prediction_list[:,1])\n",
    "df_predictions = pd.DataFrame({'Prediction': predicted_labels_1, 'Prediction2': predicted_labels_2})\n",
    "\n",
    "# Display the resulting DataFrame\n",
    "print(df_predictions)\n",
    "df_predictions.to_excel('predictions4.xlsx')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_values = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_two_probs = np.sort(predictions, axis=1)[:, -2:]\n",
    "top_two_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_two_indices = np.argsort(predictions, axis=1)[:, -2:]\n",
    "predicted_label_indices = top_two_indices[:, ::-1]\n",
    "first_predictions = predicted_label_indices[:,0]\n",
    "second_predictions = predicted_label_indices[:,1]\n",
    "labels = ['Bias', 'Drift', 'Gain', 'NoFault', 'Outliers', 'Precisiondegradation']\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(labels)\n",
    "predicted_labels_1 = label_encoder.inverse_transform(first_predictions)\n",
    "predicted_labels_2 = label_encoder.inverse_transform(second_predictions)\n",
    "df_predictions = pd.DataFrame({'Prediction': predicted_labels_1, 'Prediction2': predicted_labels_2})\n",
    "\n",
    "df_predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_predictions.to_excel('predictions.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['Bias', 'Drift', 'Gain', 'NoFault', 'Outliers', 'Precisiondegradation']\n",
    "predicted_labels = np.argmax(predictions, axis=1)\n",
    "predicted_label_indices = np.argmax(predictions, axis=1)\n",
    "\n",
    "# top_two_indices = np.argsort(predictions, axis=1)[:, -2:]\n",
    "# predicted_label_indices = top_two_indices[:, ::-1]\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(labels)\n",
    "\n",
    "predicted_labels = label_encoder.inverse_transform(predicted_label_indices)\n",
    "\n",
    "predicted_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_two_probs = np.sort(predictions, axis=1)[:, -2:]\n",
    "threshold = 0.5  # Adjust this threshold according to your needs\n",
    "\n",
    "predicted_labels = np.zeros((predictions.shape[0], 2), dtype=np.int32)\n",
    "predicted_labels[:, 0] = predicted_label_indices\n",
    "\n",
    "mask = top_two_probs[:, 1] > threshold\n",
    "predicted_labels[mask, 1] = predicted_label_indices[mask]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_two_probs = np.sort(predictions, axis=1)[:, -2:]\n",
    "threshold = 0.5  # Adjust this threshold according to your needs\n",
    "\n",
    "predicted_labels = np.zeros((predictions.shape[0], 1), dtype=np.int32)\n",
    "predicted_labels[:, 0] = predicted_label_indices[:predicted_labels.shape[0]]\n",
    "\n",
    "mask = top_two_probs[:, 1] > threshold\n",
    "\n",
    "predicted_labels_2 = np.zeros((predicted_labels.shape[0], 1), dtype=np.int32)\n",
    "predicted_labels_2[mask] = predicted_label_indices[mask].reshape(-1, 1)\n",
    "\n",
    "predicted_labels = np.concatenate((predicted_labels, predicted_labels_2), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_column = label_encoder.inverse_transform(predicted_labels[:, 0])\n",
    "prediction2_column = label_encoder.inverse_transform(predicted_labels[:, 1])\n",
    "\n",
    "predictions_df = pd.DataFrame({'Prediction': prediction_column, 'Prediction2': prediction2_column})\n",
    "print(predictions_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_df.to_excel('predictions.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_label_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_labels_df = pd.DataFrame(predicted_labels)\n",
    "predicted_labels_df.to_excel('predictions.xlsx')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
