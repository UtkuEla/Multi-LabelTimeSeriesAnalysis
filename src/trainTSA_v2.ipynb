{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iOz3DPTCthAZ",
        "outputId": "778567f8-f5b1-4850-94e8-2711d4d1c3c1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "IoX2OMuOt-jV"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Conv1D, MaxPooling1D, concatenate, GlobalMaxPooling1D, Dense, LSTM, BatchNormalization, Dropout\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import sklearn\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from datetime import datetime\n",
        "import os\n",
        "from pathlib import Path\n",
        "from sklearn.multioutput import MultiOutputClassifier\n",
        "from keras.models import Sequential\n",
        "from sklearn.model_selection import RepeatedKFold\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
        "from tensorflow.keras.metrics import Precision, Recall\n",
        "from sklearn.metrics import accuracy_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "current_folder_path = os.getcwd()\n",
        "parent_folder_path = os.path.dirname(current_folder_path)\n",
        "data_save_path = os.path.join(parent_folder_path, 'train_data')\n",
        "path = data_save_path + \"\\\\train_data_values_less300_10\"\n",
        "values_pre= pd.read_pickle(path)\n",
        "path = data_save_path + \"\\\\train_labels_less300_10\"\n",
        "labels_pre= pd.read_pickle(path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "zV7xbocswcXU"
      },
      "outputs": [],
      "source": [
        "# data_path = \"/content/drive/MyDrive/ProjectArbeit/train_data_values_300_10\"\n",
        "# label_path = \"/content/drive/MyDrive/ProjectArbeit/train_labels_300_10\"\n",
        "# test_data_path = \"/content/drive/MyDrive/ProjectArbeit/test_data_values_300_10\"\n",
        "# test_label_path = \"/content/drive/MyDrive/ProjectArbeit/test_labels_300_10\"\n",
        "# model_path = \"/content/drive/MyDrive/ProjectArbeit/models\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "88hnZOsBTl15"
      },
      "outputs": [],
      "source": [
        "labels = labels_pre.values\n",
        "values = values_pre.values\n",
        "\n",
        "\n",
        "enc = sklearn.preprocessing.OneHotEncoder(categories='auto')\n",
        "enc.fit(np.concatenate((labels), axis=0).reshape(-1, 1))\n",
        "labels = enc.transform(labels.reshape(-1, 1)).toarray()\n",
        "\n",
        "# input_shape = values.shape[1:]\n",
        "# num_classes = labels.shape[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "labels_generated = pd.read_pickle('mixed_label_data')\n",
        "values_generated = pd.read_pickle('mixed_train_data')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "labels_generated = labels_generated.values\n",
        "\n",
        "original_array = labels_generated\n",
        "\n",
        "num_columns = original_array.shape[1]\n",
        "\n",
        "identity_matrix = np.eye(num_columns, dtype=np.float64)\n",
        "\n",
        "transformed_array = original_array.dot(identity_matrix)\n",
        "\n",
        "labels = np.concatenate((labels,transformed_array),axis=0 )\n",
        "values_generated = values_generated.values\n",
        "values = np.concatenate((values,values_generated), axis = 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "S5p5T8ylUnDl"
      },
      "outputs": [],
      "source": [
        "cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
        "for train_ix, test_ix in cv.split(values):\n",
        "    # prepare data\n",
        "    X_train, X_test = values[train_ix], values[test_ix]\n",
        "    y_train, y_test = labels[train_ix], labels[test_ix]\n",
        "\n",
        "    X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))\n",
        "    X_test = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "8tirMm9CZAtP"
      },
      "outputs": [],
      "source": [
        "input_shape = X_train.shape[1:]\n",
        "num_classes = len(np.unique(np.concatenate((y_train, y_test), axis=0)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wBzsjSLZZZTv",
        "outputId": "c1e9e861-dab9-4bf0-edf2-ab54ceabf25a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "6"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "num_classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TodrdMu9YpC3",
        "outputId": "591e31f7-df6b-4c2b-c0de-79de1e884d40"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "X_train shape: (3078, 300, 1)\n",
            "X_test shape: (341, 300, 1)\n",
            "y_train shape: (3078, 1)\n",
            "y_test shape: (341, 1)\n"
          ]
        }
      ],
      "source": [
        "print(\"X_train shape:\", X_train.shape)\n",
        "print(\"X_test shape:\", X_test.shape)\n",
        "print(\"y_train shape:\", y_train.shape)\n",
        "print(\"y_test shape:\", y_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FtGTar4Fx8bT",
        "outputId": "04208a09-f9fe-49bc-a966-d30ee767017e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "X_train shape: (2735, 300, 1)\n",
            "X_test shape: (684, 300, 1)\n",
            "y_train shape: (2735, 1)\n",
            "y_test shape: (684, 1)\n"
          ]
        }
      ],
      "source": [
        "# labels = labels_pre.values\n",
        "# values = values_pre.values\n",
        "# X_train, X_test, y_train, y_test = train_test_split(values,labels, test_size=0.2, random_state=42)\n",
        "# X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))\n",
        "# X_test = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))\n",
        "\n",
        "# print(\"X_train shape:\", X_train.shape)\n",
        "# print(\"X_test shape:\", X_test.shape)\n",
        "# print(\"y_train shape:\", y_train.shape)\n",
        "# print(\"y_test shape:\", y_test.shape)\n",
        "\n",
        "# num_classes = len(np.unique(np.concatenate((y_train, y_test), axis=0)))\n",
        "# input_shape = X_train.shape[1:]\n",
        "\n",
        "# enc = sklearn.preprocessing.OneHotEncoder(categories='auto')\n",
        "# enc.fit(np.concatenate((y_train, y_test), axis=0).reshape(-1, 1))\n",
        "# y_train = enc.transform(y_train.reshape(-1, 1)).toarray()\n",
        "# y_test = enc.transform(y_test.reshape(-1, 1)).toarray()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TNI34phMgF_H",
        "outputId": "fdb3e15e-5ad6-4cc3-bf2e-612883d12a20"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(300, 1)"
            ]
          },
          "execution_count": 61,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "input_shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CAKX4FCFyO-C"
      },
      "outputs": [],
      "source": [
        "#softmax model\n",
        "input_layer = Input(shape=input_shape)\n",
        "\n",
        "conv_branch1 = Conv1D(filters=128, kernel_size=3, activation='relu')(input_layer)\n",
        "\n",
        "# Convolutional branch\n",
        "conv_branch = Conv1D(filters=64, kernel_size=3, activation='relu')(conv_branch1)\n",
        "conv_branch = MaxPooling1D(pool_size=2)(conv_branch)\n",
        "\n",
        "# First recurrent branch\n",
        "rnn_branch1 = LSTM(units=64, return_sequences=True)(conv_branch1)\n",
        "\n",
        "# Second recurrent branch\n",
        "rnn_branch2 = LSTM(units=64, return_sequences=True)(rnn_branch1)\n",
        "\n",
        "# Apply global max pooling to the convolutional branch\n",
        "conv_branch = GlobalMaxPooling1D()(conv_branch)\n",
        "\n",
        "# Apply global max pooling to the recurrent branches\n",
        "rnn_branch2 = GlobalMaxPooling1D()(rnn_branch2)\n",
        "\n",
        "# Concatenate the outputs of all branches\n",
        "concatenated = concatenate([conv_branch, rnn_branch2])\n",
        "\n",
        "# Classification layers\n",
        "dense_layer = Dense(units=128, activation='relu')(concatenated)\n",
        "output_layer = Dense(units=num_classes, activation='softmax')(dense_layer)\n",
        "\n",
        "# Create the model\n",
        "model = Model(inputs=input_layer, outputs=output_layer)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model.summary()\n",
        "tf.keras.utils.plot_model(model,'model.png', show_shapes=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Qj95zjl2TlI"
      },
      "outputs": [],
      "source": [
        "#sigmoid model\n",
        "input_layer = Input(shape=input_shape)\n",
        "\n",
        "conv_branch1 = Conv1D(filters=128, kernel_size=3, activation='relu')(input_layer)\n",
        "\n",
        "# Convolutional branch\n",
        "conv_branch = Conv1D(filters=64, kernel_size=3, activation='relu')(conv_branch1)\n",
        "conv_branch = MaxPooling1D(pool_size=2)(conv_branch)\n",
        "\n",
        "# First recurrent branch\n",
        "rnn_branch1 = LSTM(units=64, return_sequences=True)(conv_branch1)\n",
        "\n",
        "# Second recurrent branch\n",
        "rnn_branch2 = LSTM(units=64, return_sequences=True)(rnn_branch1)\n",
        "\n",
        "# Apply global max pooling to the convolutional branch\n",
        "conv_branch = GlobalMaxPooling1D()(conv_branch)\n",
        "\n",
        "# Apply global max pooling to the recurrent branches\n",
        "rnn_branch2 = GlobalMaxPooling1D()(rnn_branch2)\n",
        "\n",
        "# Concatenate the outputs of all branches\n",
        "concatenated = concatenate([conv_branch, rnn_branch2])\n",
        "\n",
        "# Classification layers\n",
        "dense_layer = Dense(units=128, activation='relu')(concatenated)\n",
        "output_layer = Dense(units=num_classes, activation='sigmoid')(dense_layer)\n",
        "\n",
        "# Create the model\n",
        "model = Model(inputs=input_layer, outputs=output_layer)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model.summary()\n",
        "tf.keras.utils.plot_model(model,'model.png', show_shapes=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "metadata": {
        "id": "LZgkEBET1U0u"
      },
      "outputs": [],
      "source": [
        "def call_sigmoid_model(input_shape, num_classes):\n",
        "    #sigmoid model\n",
        "    input_layer = Input(shape=input_shape)\n",
        "\n",
        "    conv_branch_in = Conv1D(filters=128, kernel_size=3, activation='relu')(input_layer)\n",
        "\n",
        "    # Convolutional branch\n",
        "    conv_branch = Conv1D(filters=128, kernel_size=3, activation='relu')(conv_branch_in)\n",
        "    conv_branch = MaxPooling1D(pool_size=2)(conv_branch)\n",
        "\n",
        "    conv_branch1 = Conv1D(filters=64, kernel_size=3, activation='relu')(conv_branch)\n",
        "    conv_branch1 = MaxPooling1D(pool_size=2)(conv_branch1)\n",
        "\n",
        "    # First recurrent branch\n",
        "    rnn_branch1 = LSTM(units=64, return_sequences=True)(conv_branch_in)\n",
        "\n",
        "    # Second recurrent branch\n",
        "    rnn_branch2 = LSTM(units=64, return_sequences=True)(rnn_branch1)\n",
        "\n",
        "    # Apply global max pooling to the convolutional branch\n",
        "    conv_branch1 = GlobalMaxPooling1D()(conv_branch1)\n",
        "\n",
        "    # Apply global max pooling to the recurrent branches\n",
        "    rnn_branch2 = GlobalMaxPooling1D()(rnn_branch2)\n",
        "\n",
        "    # Concatenate the outputs of all branches\n",
        "    concatenated = concatenate([conv_branch1, rnn_branch2])\n",
        "\n",
        "    # Classification layers\n",
        "    dense_layer = Dense(units=128, activation='relu')(concatenated)\n",
        "    output_layer = Dense(units=num_classes, activation='sigmoid')(dense_layer)\n",
        "\n",
        "    # Create the model\n",
        "    model = Model(inputs=input_layer, outputs=output_layer)\n",
        "\n",
        "    initial_learning_rate = 0.001\n",
        "\n",
        "    # Create an instance of the Adam optimizer with the initial learning rate\n",
        "    optimizer = Adam(learning_rate=initial_learning_rate)\n",
        "\n",
        "    # Compile the model with the optimizer and additional metrics\n",
        "    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy', Precision(), Recall()])\n",
        "\n",
        "    # Define a learning rate scheduler callback\n",
        "    lr_scheduler = ReduceLROnPlateau(factor=0.1, patience=3)\n",
        "\n",
        "    return model\n",
        "    # model.summary()\n",
        "    # tf.keras.utils.plot_model(model,'model.png', show_shapes=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [],
      "source": [
        "def call_sigmoid_model_long(input_shape, num_classes):\n",
        "    #sigmoid model longer and bigger version\n",
        "    input_layer = Input(shape=input_shape)\n",
        "   \n",
        "    conv_branch_in = Conv1D(filters=256, kernel_size=5, activation='relu')(input_layer)\n",
        "    conv_branch_in = Conv1D(filters=256, kernel_size=5, activation='relu')(conv_branch_in)\n",
        "    conv_branch_in = MaxPooling1D(pool_size=2)(conv_branch_in)\n",
        "\n",
        "    # Convolutional branch\n",
        "    conv_branch = Conv1D(filters=512, kernel_size=3, activation='relu')(conv_branch_in)\n",
        "    conv_branch = BatchNormalization()(conv_branch)\n",
        "    conv_branch = MaxPooling1D(pool_size=2)(conv_branch)\n",
        "\n",
        "    conv_branch = Conv1D(filters=256, kernel_size=3, activation='relu')(conv_branch)\n",
        "    conv_branch = BatchNormalization()(conv_branch)\n",
        "    conv_branch = MaxPooling1D(pool_size=2)(conv_branch)\n",
        "\n",
        "    conv_branch = Conv1D(filters=256, kernel_size=3, activation='relu')(conv_branch)\n",
        "    conv_branch = BatchNormalization()(conv_branch)\n",
        "    conv_branch = MaxPooling1D(pool_size=2)(conv_branch)\n",
        "    # Recurrent branch\n",
        "    rnn_branch = LSTM(units=256, return_sequences=True)(conv_branch_in)\n",
        "    rnn_branch = LSTM(units=256, return_sequences=True)(rnn_branch)\n",
        "    rnn_branch = LSTM(units=256, return_sequences=True)(rnn_branch)\n",
        "\n",
        "    # Global max pooling\n",
        "    conv_branch1 = GlobalMaxPooling1D()(conv_branch)\n",
        "    rnn_branch2 = GlobalMaxPooling1D()(rnn_branch)\n",
        "\n",
        "    # Concatente\n",
        "    concatenated = concatenate([conv_branch1, rnn_branch2])\n",
        "\n",
        "    # Output\n",
        "    dense_layer = Dense(units=128, activation='relu')(concatenated)\n",
        "    dense_layer = Dropout(0.5)(dense_layer)\n",
        "    output_layer = Dense(units=num_classes, activation='sigmoid')(dense_layer)\n",
        "\n",
        "\n",
        "    model = Model(inputs=input_layer, outputs=output_layer)\n",
        "\n",
        "    initial_learning_rate = 0.001\n",
        "\n",
        "    optimizer = Adam(learning_rate=initial_learning_rate)\n",
        "\n",
        "    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy', Precision(), Recall()])\n",
        "\n",
        "\n",
        "    return model\n",
        "    # model.summary()\n",
        "    # tf.keras.utils.plot_model(model,'model.png', show_shapes=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 300, 1)]     0           []                               \n",
            "                                                                                                  \n",
            " conv1d (Conv1D)                (None, 296, 256)     1536        ['input_1[0][0]']                \n",
            "                                                                                                  \n",
            " conv1d_1 (Conv1D)              (None, 292, 256)     327936      ['conv1d[0][0]']                 \n",
            "                                                                                                  \n",
            " max_pooling1d (MaxPooling1D)   (None, 146, 256)     0           ['conv1d_1[0][0]']               \n",
            "                                                                                                  \n",
            " conv1d_2 (Conv1D)              (None, 144, 512)     393728      ['max_pooling1d[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization (BatchNorm  (None, 144, 512)    2048        ['conv1d_2[0][0]']               \n",
            " alization)                                                                                       \n",
            "                                                                                                  \n",
            " max_pooling1d_1 (MaxPooling1D)  (None, 72, 512)     0           ['batch_normalization[0][0]']    \n",
            "                                                                                                  \n",
            " conv1d_3 (Conv1D)              (None, 70, 256)      393472      ['max_pooling1d_1[0][0]']        \n",
            "                                                                                                  \n",
            " batch_normalization_1 (BatchNo  (None, 70, 256)     1024        ['conv1d_3[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " max_pooling1d_2 (MaxPooling1D)  (None, 35, 256)     0           ['batch_normalization_1[0][0]']  \n",
            "                                                                                                  \n",
            " conv1d_4 (Conv1D)              (None, 33, 256)      196864      ['max_pooling1d_2[0][0]']        \n",
            "                                                                                                  \n",
            " lstm (LSTM)                    (None, 146, 256)     525312      ['max_pooling1d[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_2 (BatchNo  (None, 33, 256)     1024        ['conv1d_4[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " lstm_1 (LSTM)                  (None, 146, 256)     525312      ['lstm[0][0]']                   \n",
            "                                                                                                  \n",
            " max_pooling1d_3 (MaxPooling1D)  (None, 16, 256)     0           ['batch_normalization_2[0][0]']  \n",
            "                                                                                                  \n",
            " lstm_2 (LSTM)                  (None, 146, 256)     525312      ['lstm_1[0][0]']                 \n",
            "                                                                                                  \n",
            " global_max_pooling1d (GlobalMa  (None, 256)         0           ['max_pooling1d_3[0][0]']        \n",
            " xPooling1D)                                                                                      \n",
            "                                                                                                  \n",
            " global_max_pooling1d_1 (Global  (None, 256)         0           ['lstm_2[0][0]']                 \n",
            " MaxPooling1D)                                                                                    \n",
            "                                                                                                  \n",
            " concatenate (Concatenate)      (None, 512)          0           ['global_max_pooling1d[0][0]',   \n",
            "                                                                  'global_max_pooling1d_1[0][0]'] \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 128)          65664       ['concatenate[0][0]']            \n",
            "                                                                                                  \n",
            " dropout (Dropout)              (None, 128)          0           ['dense[0][0]']                  \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, 4)            516         ['dropout[0][0]']                \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 2,959,748\n",
            "Trainable params: 2,957,700\n",
            "Non-trainable params: 2,048\n",
            "__________________________________________________________________________________________________\n",
            "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model to work.\n"
          ]
        }
      ],
      "source": [
        "model = call_sigmoid_model_long((300, 1),4)\n",
        "model.summary()\n",
        "tf.keras.utils.plot_model(model,'model.png', show_shapes=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jG4i6iH4aboM"
      },
      "outputs": [],
      "source": [
        "# define the model\n",
        "model = Sequential()\n",
        "model.add(Dense(20, input_dim=300, kernel_initializer='he_uniform', activation='relu'))\n",
        "model.add(Dense(num_classes, activation='sigmoid'))\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZfH1f7lichWN",
        "outputId": "f9a0cb3a-2db4-4bd1-ff20-bec7fc7e83ff"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "X_train shape: (6156, 300, 1)\n",
            "X_test shape: (684, 300, 1)\n",
            "y_train shape: (6156, 4)\n",
            "y_test shape: (684, 4)\n",
            "Epoch 1/15\n",
            " 66/193 [=========>....................] - ETA: 4:11 - loss: 0.5142 - accuracy: 0.5156 - precision_1: 0.7055 - recall_1: 0.6484"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32mc:\\Users\\UTKU\\OneDrive\\Desktop\\MS-Term4\\ProjectArbeit\\src\\trainTSA_v2.ipynb Cell 21\u001b[0m in \u001b[0;36m<cell line: 35>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/UTKU/OneDrive/Desktop/MS-Term4/ProjectArbeit/src/trainTSA_v2.ipynb#X24sZmlsZQ%3D%3D?line=47'>48</a>\u001b[0m model \u001b[39m=\u001b[39m call_sigmoid_model_long(n_inputs, n_outputs)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/UTKU/OneDrive/Desktop/MS-Term4/ProjectArbeit/src/trainTSA_v2.ipynb#X24sZmlsZQ%3D%3D?line=48'>49</a>\u001b[0m \u001b[39m# fit model\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/UTKU/OneDrive/Desktop/MS-Term4/ProjectArbeit/src/trainTSA_v2.ipynb#X24sZmlsZQ%3D%3D?line=49'>50</a>\u001b[0m history \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mfit(X_train, y_train, batch_size\u001b[39m=\u001b[39;49mbatch_size, epochs\u001b[39m=\u001b[39;49mnb_epochs, callbacks\u001b[39m=\u001b[39;49m[lr_scheduler])\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/UTKU/OneDrive/Desktop/MS-Term4/ProjectArbeit/src/trainTSA_v2.ipynb#X24sZmlsZQ%3D%3D?line=50'>51</a>\u001b[0m \u001b[39m# make a prediction on the test set\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/UTKU/OneDrive/Desktop/MS-Term4/ProjectArbeit/src/trainTSA_v2.ipynb#X24sZmlsZQ%3D%3D?line=51'>52</a>\u001b[0m yhat \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mpredict(X_test)\n",
            "File \u001b[1;32mc:\\Users\\UTKU\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
            "File \u001b[1;32mc:\\Users\\UTKU\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py:1650\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1642\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[0;32m   1643\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   1644\u001b[0m     epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1647\u001b[0m     _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[0;32m   1648\u001b[0m ):\n\u001b[0;32m   1649\u001b[0m     callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1650\u001b[0m     tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[0;32m   1651\u001b[0m     \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[0;32m   1652\u001b[0m         context\u001b[39m.\u001b[39masync_wait()\n",
            "File \u001b[1;32mc:\\Users\\UTKU\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
            "File \u001b[1;32mc:\\Users\\UTKU\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:880\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    877\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    879\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 880\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m    882\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    883\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
            "File \u001b[1;32mc:\\Users\\UTKU\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:912\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    909\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[0;32m    910\u001b[0m   \u001b[39m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    911\u001b[0m   \u001b[39m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 912\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_no_variable_creation_fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)  \u001b[39m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    913\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_variable_creation_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    914\u001b[0m   \u001b[39m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    915\u001b[0m   \u001b[39m# in parallel.\u001b[39;00m\n\u001b[0;32m    916\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n",
            "File \u001b[1;32mc:\\Users\\UTKU\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compiler.py:134\u001b[0m, in \u001b[0;36mTracingCompiler.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    131\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m    132\u001b[0m   (concrete_function,\n\u001b[0;32m    133\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m--> 134\u001b[0m \u001b[39mreturn\u001b[39;00m concrete_function\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[0;32m    135\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39;49mconcrete_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n",
            "File \u001b[1;32mc:\\Users\\UTKU\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\monomorphic_function.py:1745\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1741\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1742\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1743\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1744\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1745\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function\u001b[39m.\u001b[39;49mcall(\n\u001b[0;32m   1746\u001b[0m       ctx, args, cancellation_manager\u001b[39m=\u001b[39;49mcancellation_manager))\n\u001b[0;32m   1747\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1748\u001b[0m     args,\n\u001b[0;32m   1749\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1750\u001b[0m     executing_eagerly)\n\u001b[0;32m   1751\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
            "File \u001b[1;32mc:\\Users\\UTKU\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\monomorphic_function.py:378\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    376\u001b[0m \u001b[39mwith\u001b[39;00m _InterpolateFunctionError(\u001b[39mself\u001b[39m):\n\u001b[0;32m    377\u001b[0m   \u001b[39mif\u001b[39;00m cancellation_manager \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 378\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[0;32m    379\u001b[0m         \u001b[39mstr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msignature\u001b[39m.\u001b[39;49mname),\n\u001b[0;32m    380\u001b[0m         num_outputs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_num_outputs,\n\u001b[0;32m    381\u001b[0m         inputs\u001b[39m=\u001b[39;49margs,\n\u001b[0;32m    382\u001b[0m         attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[0;32m    383\u001b[0m         ctx\u001b[39m=\u001b[39;49mctx)\n\u001b[0;32m    384\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    385\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    386\u001b[0m         \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msignature\u001b[39m.\u001b[39mname),\n\u001b[0;32m    387\u001b[0m         num_outputs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    390\u001b[0m         ctx\u001b[39m=\u001b[39mctx,\n\u001b[0;32m    391\u001b[0m         cancellation_manager\u001b[39m=\u001b[39mcancellation_manager)\n",
            "File \u001b[1;32mc:\\Users\\UTKU\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:52\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 52\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[0;32m     53\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     54\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     55\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# data_path = \"/content/drive/MyDrive/ProjectArbeit/train_data_values_less300_10\"\n",
        "# label_path = \"/content/drive/MyDrive/ProjectArbeit/train_labels_less300_10\"\n",
        "# test_data_path = \"/content/drive/MyDrive/ProjectArbeit/test_data_values_less300_10\"\n",
        "# test_label_path = \"/content/drive/MyDrive/ProjectArbeit/test_labels_less300_10\"\n",
        "# model_path = \"/content/drive/MyDrive/ProjectArbeit/models\"\n",
        "\n",
        "# current_folder_path = os.getcwd()\n",
        "# parent_folder_path = os.path.dirname(current_folder_path)\n",
        "# data_save_path = os.path.join(parent_folder_path, 'train_data')\n",
        "# path = data_save_path + \"\\\\train_data_values_less300_10\"\n",
        "# values_pre= pd.read_pickle(path)\n",
        "# path = data_save_path + \"\\\\train_labels_less300_10\"\n",
        "# labels_pre= pd.read_pickle(path)\n",
        "\n",
        "# # values_pre= pd.read_pickle(data_path)\n",
        "# # labels_pre= pd.read_pickle(label_path)\n",
        "\n",
        "# labels = labels_pre.values\n",
        "# values = values_pre.values\n",
        "\n",
        "# enc = sklearn.preprocessing.OneHotEncoder(categories='auto')\n",
        "# enc.fit(np.concatenate((labels), axis=0).reshape(-1, 1))\n",
        "# labels = enc.transform(labels.reshape(-1, 1)).toarray()\n",
        "\n",
        "X = values\n",
        "y = labels\n",
        "batch_size = 32\n",
        "nb_epochs = 15\n",
        "n_inputs, n_outputs = (300, 1) , 4\n",
        "results = list()\n",
        "lr_scheduler = ReduceLROnPlateau(factor=0.1, patience=10, monitor='loss')\n",
        "# define evaluation procedure\n",
        "cv = RepeatedKFold(n_splits=10, n_repeats=1, random_state=1)\n",
        "\n",
        "for train_ix, test_ix in cv.split(values):\n",
        "    # prepare data\n",
        "    X_train, X_test = X[train_ix], X[test_ix]\n",
        "    y_train, y_test = y[train_ix], y[test_ix]\n",
        "\n",
        "    X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))\n",
        "    X_test = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))\n",
        "\n",
        "    print(\"X_train shape:\", X_train.shape)\n",
        "    print(\"X_test shape:\", X_test.shape)\n",
        "    print(\"y_train shape:\", y_train.shape)\n",
        "    print(\"y_test shape:\", y_test.shape)\n",
        "    # define model\n",
        "    model = call_sigmoid_model_long(n_inputs, n_outputs)\n",
        "    # fit model\n",
        "    history = model.fit(X_train, y_train, batch_size=batch_size, epochs=nb_epochs, callbacks=[lr_scheduler])\n",
        "    # make a prediction on the test set\n",
        "    yhat = model.predict(X_test)\n",
        "    # round probabilities to class labels\n",
        "    yhat = yhat.round()\n",
        "    # calculate accuracy\n",
        "    acc = accuracy_score(y_test, yhat)\n",
        "    # store result\n",
        "    print('>%.3f' % acc)\n",
        "    results.append(acc)\n",
        "\n",
        "\n",
        "print('Accuracy: %.3f (%.3f)' % (np.mean(results), np.std(results)))\n",
        "modelName= 'Sigmoid_long'\n",
        "model_path = r\"C:\\Users\\UTKU\\OneDrive\\Desktop\\MS-Term4\\ProjectArbeit\\models\"\n",
        "now = datetime.now()\n",
        "timestamp = now.strftime(\"%d-%m-%H-%M\")\n",
        "model_filename = model_path + '/' + modelName + f\"_{timestamp}.h5\"\n",
        "model.save(model_filename)\n",
        "print(f\"Model saved as {model_filename}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {},
      "outputs": [],
      "source": [
        "values_pre.to_excel(\"300_less_value.xlsx\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.990 (0.008)\n",
            "Model saved as C:/Users/UTKU/OneDrive/Desktop/MS-Term4/ProjectArbeit/models/Sigmoid_with_RKFold_and_lr_15-07-23-54.h5\n"
          ]
        }
      ],
      "source": [
        "print('Accuracy: %.3f (%.3f)' % (np.mean(results), np.std(results)))\n",
        "modelName= 'Sigmoid_with_RKFold_and_lr'\n",
        "model_path = \"C:/Users/UTKU/OneDrive/Desktop/MS-Term4/ProjectArbeit/models\"\n",
        "now = datetime.now()\n",
        "timestamp = now.strftime(\"%d-%m-%H-%M\")\n",
        "model_filename = model_path + '/' + modelName + f\"_{timestamp}.h5\"\n",
        "model.save(model_filename)\n",
        "print(f\"Model saved as {model_filename}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[0.9963503649635036,\n",
              " 0.9890510948905109,\n",
              " 1.0,\n",
              " 0.9744525547445255,\n",
              " 0.9963503649635036,\n",
              " 0.9817518248175182,\n",
              " 0.9963369963369964,\n",
              " 0.9853479853479854,\n",
              " 0.9963369963369964,\n",
              " 0.9853479853479854]"
            ]
          },
          "execution_count": 94,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vL5ECUH6tqZU",
        "outputId": "6acdefaa-d685-498c-cb56-d1a07965b7a8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model saved as /content/drive/MyDrive/ProjectArbeit/models/Sigmoid_with_RKFold_and_lr_14-07-14-03.h5\n"
          ]
        }
      ],
      "source": [
        "modelName= 'Sigmoid_with_RKFold_and_lr'\n",
        "now = datetime.now()\n",
        "timestamp = now.strftime(\"%d-%m-%H-%M\")\n",
        "model_filename = model_path + '/' + modelName + f\"_{timestamp}.h5\"\n",
        "model.save(model_filename)\n",
        "print(f\"Model saved as {model_filename}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 796
        },
        "id": "uXSq9PHfz1GQ",
        "outputId": "2ae5e98d-5ee1-4ae7-8209-66194e62df28"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/500\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-38-b32cdaeb4179>\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mlr_scheduler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mReduceLROnPlateau\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfactor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpatience\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     history = model.fit(X_train, y_train, batch_size=batch_size, epochs=nb_epochs,\n\u001b[0m\u001b[1;32m     24\u001b[0m                     validation_data=(X_test, y_test), callbacks=[lr_scheduler])\n\u001b[1;32m     25\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtf__train_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m                     \u001b[0mretval_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep_function\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m                 \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\", line 1284, in train_function  *\n        return step_function(self, iterator)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\", line 1268, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\", line 1249, in run_step  **\n        outputs = model.train_step(data)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\", line 1051, in train_step\n        loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\", line 1109, in compute_loss\n        return self.compiled_loss(\n    File \"/usr/local/lib/python3.10/dist-packages/keras/engine/compile_utils.py\", line 265, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/losses.py\", line 142, in __call__\n        losses = call_fn(y_true, y_pred)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/losses.py\", line 268, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/losses.py\", line 2156, in binary_crossentropy\n        backend.binary_crossentropy(y_true, y_pred, from_logits=from_logits),\n    File \"/usr/local/lib/python3.10/dist-packages/keras/backend.py\", line 5707, in binary_crossentropy\n        return tf.nn.sigmoid_cross_entropy_with_logits(\n\n    ValueError: `logits` and `labels` must have the same shape, received ((None, 6) vs (None, 1)).\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "earlystopping = False\n",
        "lr_scheduler = True\n",
        "modelName= 'Model'\n",
        "\n",
        "batch_size = 32\n",
        "nb_epochs = 500\n",
        "\n",
        "if earlystopping:\n",
        "    # Define early stopping callback\n",
        "    early_stopping = EarlyStopping(monitor='val_loss', patience=75, restore_best_weights=True)\n",
        "\n",
        "    # Compile the model\n",
        "    #model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "    # Train the model with optimizations\n",
        "    history = model.fit(X_train, y_train, batch_size=batch_size, epochs=nb_epochs,\n",
        "                    validation_data=(X_test, y_test), callbacks=[early_stopping])\n",
        "\n",
        "else:\n",
        "   history = model.fit(X_train, y_train, batch_size=batch_size, epochs=nb_epochs,\n",
        "                    validation_data=(X_test, y_test))\n",
        "\n",
        "\n",
        "now = datetime.now()\n",
        "timestamp = now.strftime(\"%d-%m-%H-%M\")\n",
        "model_filename = model_path + '/' + modelName + f\"_{timestamp}.h5\"\n",
        "model.save(model_filename)\n",
        "print(f\"Model saved as {model_filename}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2wp-Q_ecHTVr"
      },
      "outputs": [],
      "source": [
        "model_path1 = model_path + '/' + 'P_RNN_CNN_sigmoid3__10-07-22-21.h5'\n",
        "model = tf.keras.models.load_model(model_path1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 165
        },
        "id": "DGiZtqyE2D9B",
        "outputId": "20dd1e14-918b-4ae3-8266-30da32d09c95"
      },
      "outputs": [
        {
          "ename": "KeyError",
          "evalue": "'val_accuracy'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[1;32mc:\\Users\\UTKU\\OneDrive\\Desktop\\MS-Term4\\ProjectArbeit\\src\\trainTSA_v2.ipynb Cell 27\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/UTKU/OneDrive/Desktop/MS-Term4/ProjectArbeit/src/trainTSA_v2.ipynb#X33sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m visualize(history, \u001b[39m199\u001b[39;49m, modelName)\n",
            "\u001b[1;32mc:\\Users\\UTKU\\OneDrive\\Desktop\\MS-Term4\\ProjectArbeit\\src\\trainTSA_v2.ipynb Cell 27\u001b[0m in \u001b[0;36mvisualize\u001b[1;34m(history, epochs, title)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/UTKU/OneDrive/Desktop/MS-Term4/ProjectArbeit/src/trainTSA_v2.ipynb#X33sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mvisualize\u001b[39m(history, epochs, title):\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/UTKU/OneDrive/Desktop/MS-Term4/ProjectArbeit/src/trainTSA_v2.ipynb#X33sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     acc \u001b[39m=\u001b[39m history\u001b[39m.\u001b[39mhistory[\u001b[39m'\u001b[39m\u001b[39maccuracy\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/UTKU/OneDrive/Desktop/MS-Term4/ProjectArbeit/src/trainTSA_v2.ipynb#X33sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     val_acc \u001b[39m=\u001b[39m history\u001b[39m.\u001b[39;49mhistory[\u001b[39m'\u001b[39;49m\u001b[39mval_accuracy\u001b[39;49m\u001b[39m'\u001b[39;49m]\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/UTKU/OneDrive/Desktop/MS-Term4/ProjectArbeit/src/trainTSA_v2.ipynb#X33sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     loss \u001b[39m=\u001b[39m history\u001b[39m.\u001b[39mhistory[\u001b[39m'\u001b[39m\u001b[39mloss\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/UTKU/OneDrive/Desktop/MS-Term4/ProjectArbeit/src/trainTSA_v2.ipynb#X33sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     val_loss \u001b[39m=\u001b[39m history\u001b[39m.\u001b[39mhistory[\u001b[39m'\u001b[39m\u001b[39mval_loss\u001b[39m\u001b[39m'\u001b[39m]\n",
            "\u001b[1;31mKeyError\u001b[0m: 'val_accuracy'"
          ]
        }
      ],
      "source": [
        "visualize(history, 199, modelName)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7gASdKWvz1BI",
        "outputId": "5b0ebe53-ea8d-4ac8-86c7-8f4819694123"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model saved as /content/drive/MyDrive/ProjectArbeit/models/P_RNN_CNN_sigmoid2__10-07-19-02.h5\n"
          ]
        }
      ],
      "source": [
        "now = datetime.now()\n",
        "timestamp = now.strftime(\"%d-%m-%H-%M\")\n",
        "model_filename = model_path + '/' + modelName + '_' + f\"_{timestamp}.h5\"\n",
        "model.save(model_filename)\n",
        "print(f\"Model saved as {model_filename}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dmN809rLAJD0",
        "outputId": "abab6f8b-1028-4a42-9d49-f3f122daec61"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "64/64 [==============================] - 7s 113ms/step\n"
          ]
        }
      ],
      "source": [
        "\n",
        "test_data_path = \"C:/Users/UTKU/OneDrive/Desktop/MS-Term4/ProjectArbeit/test_data/test_data_values_less300_10\"\n",
        "test_label_path = \"C:/Users/UTKU/OneDrive/Desktop/MS-Term4/ProjectArbeit/test_data/test_data_labels_less300_10\"\n",
        "\n",
        "test_values = pd.read_pickle(test_data_path)\n",
        "test_labels = pd.read_pickle(test_label_path)\n",
        "\n",
        "test_values = test_values.values\n",
        "test_values = test_values.reshape((test_values.shape[0], test_values.shape[1], 1))\n",
        "\n",
        "predictions = model.predict(test_values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_label_path = \"C:/Users/UTKU/OneDrive/Desktop/MS-Term4/ProjectArbeit/test_data/test_labels_less300_10\"\n",
        "\n",
        "test_labels = pd.read_pickle(test_label_path)\n",
        "test_labels.to_excel('test_data_labels_less300_10.xlsx')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[3.6781836e-01, 5.7966161e-01, 1.3927696e-09, 3.1160231e-04],\n",
              "       [4.0128991e-02, 9.8508090e-01, 4.7613300e-09, 4.7525125e-05],\n",
              "       [4.8066914e-04, 9.9985886e-01, 4.6288317e-08, 1.1577538e-05],\n",
              "       ...,\n",
              "       [9.2849914e-05, 1.9537350e-03, 3.0941242e-01, 6.8775696e-01],\n",
              "       [2.6745041e-05, 8.8108925e-04, 5.7322556e-01, 4.4810566e-01],\n",
              "       [1.0577276e-04, 2.1574064e-03, 2.8284380e-01, 7.0976216e-01]],\n",
              "      dtype=float32)"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ptuv-yBTz0-o",
        "outputId": "c1d68874-79d6-4859-c6be-df045dd7864e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                Prediction           Prediction2\n",
            "0                    Drift                  Bias\n",
            "1                    Drift               NoFault\n",
            "2                    Drift               NoFault\n",
            "3                    Drift               NoFault\n",
            "4                    Drift               NoFault\n",
            "...                    ...                   ...\n",
            "2041  Precisiondegradation               NoFault\n",
            "2042  Precisiondegradation               NoFault\n",
            "2043  Precisiondegradation                  Gain\n",
            "2044                  Gain  Precisiondegradation\n",
            "2045  Precisiondegradation               NoFault\n",
            "\n",
            "[2046 rows x 2 columns]\n"
          ]
        }
      ],
      "source": [
        "prediction_list = []\n",
        "\n",
        "threshold = 0.3\n",
        "# Iterate over each row of predictions\n",
        "for row in predictions:\n",
        "\n",
        "    sorted_indices = np.argsort(row)[::-1]\n",
        "    prediction1 = sorted_indices[0]\n",
        "\n",
        "    if row[sorted_indices[1]] >= threshold:\n",
        "        prediction2 = sorted_indices[1]\n",
        "    else:\n",
        "        prediction2 = 4\n",
        "\n",
        "    prediction_list.append([prediction1, prediction2])\n",
        "\n",
        "prediction_list = np.array(prediction_list)\n",
        "\n",
        "def convert_array_to_labels(array):\n",
        "    labels1 = ['Bias','Drift','Gain','Precisiondegradation','NoFault']\n",
        "    converted_array = [labels1[i] for i in array]\n",
        "    return converted_array\n",
        "\n",
        "predicted_labels_1 = convert_array_to_labels(prediction_list[:, 0])\n",
        "predicted_labels_2 = convert_array_to_labels(prediction_list[:, 1])\n",
        "\n",
        "# label_encoder = LabelEncoder()\n",
        "# label_encoder.fit(labels1)\n",
        "# predicted_labels_1 = label_encoder.inverse_transform(prediction_list[:, 0])\n",
        "# predicted_labels_2 = label_encoder.inverse_transform(prediction_list[:, 1])\n",
        "df_predictions = pd.DataFrame({'Prediction': predicted_labels_1, 'Prediction2': predicted_labels_2})\n",
        "\n",
        "now = datetime.now()\n",
        "timestamp = now.strftime(\"%d-%m-%H-%M\")\n",
        "prediction_name = model_path + '/' + modelName + f\"_{timestamp}\" + f\"_{threshold}_\" + \"_predictions.xlsx\"\n",
        "print(df_predictions)\n",
        "df_predictions.to_excel(prediction_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {},
      "outputs": [],
      "source": [
        "current_folder_path = os.getcwd()\n",
        "parent_folder_path = os.path.dirname(current_folder_path)\n",
        "data_save_path = os.path.join(parent_folder_path, 'train_data')\n",
        "path = data_save_path + \"\\\\train_data_values_less300_10\"\n",
        "values_pre= pd.read_pickle(path)\n",
        "path = data_save_path + \"\\\\train_labels_less300_10\"\n",
        "labels_pre= pd.read_pickle(path)\n",
        "\n",
        "# values_pre= pd.read_pickle(data_path)\n",
        "# labels_pre= pd.read_pickle(label_path)\n",
        "\n",
        "labels = labels_pre.values\n",
        "values = values_pre.values\n",
        "\n",
        "enc = sklearn.preprocessing.OneHotEncoder(categories='auto')\n",
        "enc.fit(np.concatenate((labels), axis=0).reshape(-1, 1))\n",
        "labels = enc.transform(labels.reshape(-1, 1)).toarray()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([0, 4, 4, ..., 2, 3, 2], dtype=int64)"
            ]
          },
          "execution_count": 66,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "prediction_list[:, 1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mnWk2AfAz0qp",
        "outputId": "fb72ff07-e904-40f1-93ff-090ec43409f3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1.0577276e-04 2.1574064e-03 2.8284380e-01 7.0976216e-01]\n",
            "[3 2]\n",
            "Prediction     Precisiondegradation\n",
            "Prediction2                    Gain\n",
            "Name: 2045, dtype: object\n",
            "Precisiondegradation\n",
            "Gain\n"
          ]
        }
      ],
      "source": [
        "line = 2045\n",
        "print(predictions[line])\n",
        "print(prediction_list[line])\n",
        "print(df_predictions.iloc[line])\n",
        "print(predicted_labels_1[line])\n",
        "print(predicted_labels_2[line])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Precisiondegradation\n"
          ]
        }
      ],
      "source": [
        "print(predicted_labels_2[716])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "5K9ppzbHy1-7"
      },
      "outputs": [],
      "source": [
        "def visualize(history, epochs, title):\n",
        "    acc = history.history['accuracy']\n",
        "    val_acc = history.history['val_accuracy']\n",
        "\n",
        "    loss = history.history['loss']\n",
        "    val_loss = history.history['val_loss']\n",
        "\n",
        "    epochs_range = range(epochs)\n",
        "\n",
        "    plt.figure(figsize=(8, 8))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(epochs_range, acc, label='Training Accuracy')\n",
        "    plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
        "    plt.legend(loc='lower right')\n",
        "    plt.title(title + ' Accuracy')\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(epochs_range, loss, label='Training Loss')\n",
        "    plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
        "    plt.legend(loc='upper right')\n",
        "    plt.title(title + ' Loss')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "3KaHpTqViqm6"
      },
      "outputs": [],
      "source": [
        "# mlp for multi-label classification\n",
        "from numpy import mean\n",
        "from numpy import std\n",
        "from sklearn.datasets import make_multilabel_classification\n",
        "from sklearn.model_selection import RepeatedKFold\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# get the dataset\n",
        "def get_dataset():\n",
        "\tX, y = make_multilabel_classification(n_samples=1000, n_features=10, n_classes=3, n_labels=2, random_state=1)\n",
        "\treturn X, y\n",
        "\n",
        "# get the model\n",
        "def get_model(n_inputs, n_outputs):\n",
        "\tmodel = Sequential()\n",
        "\tmodel.add(Dense(20, input_dim=n_inputs, kernel_initializer='he_uniform', activation='relu'))\n",
        "\tmodel.add(Dense(n_outputs, activation='sigmoid'))\n",
        "\tmodel.compile(loss='binary_crossentropy', optimizer='adam')\n",
        "\treturn model\n",
        "\n",
        "# evaluate a model using repeated k-fold cross-validation\n",
        "def evaluate_model(X, y):\n",
        "\tresults = list()\n",
        "\tn_inputs, n_outputs = X.shape[1], y.shape[1]\n",
        "\t# define evaluation procedure\n",
        "\tcv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
        "\t# enumerate folds\n",
        "\tfor train_ix, test_ix in cv.split(X):\n",
        "\t\t# prepare data\n",
        "\t\tX_train, X_test = X[train_ix], X[test_ix]\n",
        "\t\ty_train, y_test = y[train_ix], y[test_ix]\n",
        "\t\t# define model\n",
        "\t\tmodel = get_model(n_inputs, n_outputs)\n",
        "\t\t# fit model\n",
        "\t\tmodel.fit(X_train, y_train, verbose=0, epochs=100)\n",
        "\t\t# make a prediction on the test set\n",
        "\t\tyhat = model.predict(X_test)\n",
        "\t\t# round probabilities to class labels\n",
        "\t\tyhat = yhat.round()\n",
        "\t\t# calculate accuracy\n",
        "\t\tacc = accuracy_score(y_test, yhat)\n",
        "\t\t# store result\n",
        "\t\tprint('>%.3f' % acc)\n",
        "\t\tresults.append(acc)\n",
        "\treturn results\n",
        "\n",
        "# load dataset\n",
        "X, y = get_dataset()\n",
        "# # evaluate model\n",
        "# results = evaluate_model(X, y)\n",
        "# # summarize performance\n",
        "# print('Accuracy: %.3f (%.3f)' % (mean(results), std(results)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tYwfNMLgoh_a",
        "outputId": "be089263-bb2a-451d-e8ce-5e25375a4c02"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[1, 1, 0],\n",
              "       [0, 0, 0],\n",
              "       [1, 1, 0],\n",
              "       ...,\n",
              "       [1, 1, 0],\n",
              "       [1, 1, 0],\n",
              "       [1, 1, 1]])"
            ]
          },
          "execution_count": 44,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2f1RBtXblCMN"
      },
      "outputs": [],
      "source": [
        "y_train1 = y_train[:][0]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nnvivSXflKfj",
        "outputId": "c21c5533-8c68-43a3-db95-c71c39ccb13a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([3])"
            ]
          },
          "execution_count": 45,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y_train1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k7FTkL07kBME"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "model = LogisticRegression(multi_class='ovr')\n",
        "# fit model\n",
        "model.fit(X_train, y_train1)\n",
        "# make predictions\n",
        "yhat = model.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2TGVdVO3mu-s"
      },
      "outputs": [],
      "source": [
        "y_test1 = y_test.flatten()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "keW4AhTlnJZc",
        "outputId": "f18f774a-4a3f-4c11-cb6b-cf33b3258845"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The element-wise similarity between the arrays is: 591\n"
          ]
        }
      ],
      "source": [
        "# Calculate element-wise similarity\n",
        "similarity = np.sum(y_test1 == yhat)\n",
        "\n",
        "591/len(yhat) *100\n",
        "# Print the similarity value\n",
        "print(f\"The element-wise similarity between the arrays is: {similarity}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KTwTFumKn9Vz",
        "outputId": "3ac7626a-bd04-422a-90f2-c30025b7ba83"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "86.40350877192982"
            ]
          },
          "execution_count": 64,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "similarity/len(yhat) *100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "McQ3AaEApIYw",
        "outputId": "26b886cc-bc76-4d9a-889d-1041f7b4c7de"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([3, 6, 1, 4, 2, 2, 3, 1, 6, 3, 3, 4, 1, 2, 1, 4, 6, 3, 4, 4, 6, 6,\n",
              "       2, 4, 1, 3, 3, 3, 4, 4, 2, 6, 4, 1, 1, 1, 2, 4, 3, 1, 2, 2, 4, 3,\n",
              "       4, 4, 4, 3, 3, 2, 2, 3, 1, 4, 2, 2, 1, 3, 4, 6, 4, 3, 1, 2, 2, 4,\n",
              "       4, 4, 2, 6, 4, 1, 1, 2, 3, 4, 1, 4, 3, 6, 3, 6, 3, 2, 2, 4, 1, 3,\n",
              "       3, 6, 6, 1, 3, 4, 2, 2, 3, 4, 4, 1, 4, 2, 4, 1, 1, 1, 4, 6, 4, 1,\n",
              "       2, 4, 4, 6, 1, 6, 2, 1, 1, 3, 4, 1, 6, 3, 1, 2, 4, 2, 2, 2, 4, 3,\n",
              "       6, 3, 1, 4, 3, 1, 1, 3, 1, 4, 4, 6, 4, 1, 2, 1, 2, 1, 4, 6, 3, 3,\n",
              "       4, 4, 4, 4, 4, 4, 1, 6, 4, 3, 1, 2, 3, 3, 4, 6, 2, 3, 1, 1, 2, 1,\n",
              "       1, 4, 1, 1, 3, 6, 4, 4, 2, 4, 2, 3, 1, 1, 3, 2, 3, 2, 2, 2, 4, 4,\n",
              "       2, 4, 2, 4, 1, 2, 4, 4, 2, 6, 2, 3, 3, 1, 4, 1, 3, 3, 1, 1, 2, 1,\n",
              "       4, 4, 6, 3, 1, 1, 1, 3, 6, 4, 2, 1, 6, 1, 3, 4, 4, 4, 4, 1, 3, 3,\n",
              "       3, 4, 2, 6, 4, 1, 1, 1, 2, 1, 4, 2, 4, 3, 4, 3, 4, 3, 3, 3, 4, 4,\n",
              "       3, 2, 2, 3, 3, 1, 3, 4, 4, 3, 2, 4, 3, 4, 1, 6, 4, 4, 2, 2, 2, 4,\n",
              "       3, 1, 1, 4, 2, 4, 4, 1, 2, 4, 3, 1, 3, 2, 3, 1, 4, 1, 4, 4, 2, 4,\n",
              "       4, 1, 4, 1, 1, 1, 1, 3, 4, 4, 3, 4, 2, 2, 3, 3, 6, 4, 1, 1, 1, 1,\n",
              "       1, 1, 3, 2, 3, 1, 4, 3, 3, 2, 1, 2, 1, 1, 4, 3, 2, 2, 1, 3, 4, 6,\n",
              "       1, 2, 4, 4, 3, 4, 3, 3, 6, 3, 1, 4, 1, 2, 4, 4, 4, 6, 6, 6, 1, 1,\n",
              "       3, 1, 1, 3, 2, 2, 4, 1, 1, 3, 3, 4, 4, 3, 2, 2, 4, 1, 4, 1, 4, 1,\n",
              "       4, 1, 3, 2, 3, 3, 4, 4, 1, 1, 1, 6, 2, 4, 2, 1, 2, 4, 1, 4, 6, 2,\n",
              "       3, 4, 1, 6, 2, 3, 4, 2, 2, 1, 3, 2, 4, 3, 2, 2, 3, 1, 3, 2, 3, 4,\n",
              "       1, 4, 2, 1, 1, 3, 4, 2, 1, 4, 1, 6, 1, 1, 6, 4, 4, 3, 4, 3, 4, 1,\n",
              "       1, 2, 1, 3, 4, 1, 3, 6, 3, 3, 2, 6, 4, 4, 4, 2, 1, 1, 1, 1, 2, 2,\n",
              "       4, 2, 2, 4, 1, 2, 1, 4, 4, 4, 1, 1, 3, 2, 4, 3, 3, 6, 4, 4, 1, 4,\n",
              "       1, 1, 2, 1, 1, 3, 3, 3, 6, 3, 4, 4, 4, 4, 2, 2, 3, 2, 1, 1, 3, 3,\n",
              "       4, 3, 3, 1, 2, 4, 3, 2, 6, 3, 4, 4, 1, 2, 1, 4, 4, 3, 4, 3, 1, 4,\n",
              "       6, 4, 3, 4, 4, 4, 3, 1, 2, 6, 4, 2, 3, 4, 1, 3, 4, 3, 3, 3, 1, 4,\n",
              "       6, 4, 4, 1, 3, 2, 4, 4, 4, 3, 4, 4, 4, 2, 3, 3, 2, 1, 1, 1, 4, 6,\n",
              "       2, 1, 1, 3, 1, 2, 4, 4, 1, 3, 2, 4, 4, 1, 3, 1, 4, 3, 4, 4, 4, 3,\n",
              "       1, 1, 4, 2, 6, 1, 4, 3, 4, 2, 2, 1, 1, 4, 6, 2, 2, 3, 1, 4, 4, 4,\n",
              "       2, 4, 1, 4, 2, 1, 3, 4, 3, 2, 2, 1, 4, 2, 1, 4, 6, 4, 1, 4, 6, 3,\n",
              "       3, 2, 4, 1, 3, 2, 1, 1, 3, 2, 6, 4, 4, 3, 2, 6, 2, 3, 2, 2, 1, 4,\n",
              "       2, 2])"
            ]
          },
          "execution_count": 67,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "yhat"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
